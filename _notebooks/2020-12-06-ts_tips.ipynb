{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 tips for working with time series in Python\n",
    "> At some point in his/her career, any Data Scientist has to be able to manipulate time series data. I have been working as a Data Scientist and Quant Researcher for the last 14 months and I found little “cooking tips” for working with this type of data. Today, I would like to share some of those tips.\n",
    "\n",
    "- toc:true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Alejandro PS\n",
    "- categories: [python, numpy, pandas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a repost from my medium article [5 tips for working with time series in Python](https://alejandrops.medium.com/5-tips-for-working-with-time-series-in-python-d889109e676d)*\n",
    "\n",
    "---\n",
    "\n",
    "## Required knowledge\n",
    "This post is pretty easy to follow if you already have some basic knowledge of Pandas, NumPy and Python. I will not go into much details with the theoretical stuff but use the resources at the end or ask a question if you need clarification about some particular concept.\n",
    "\n",
    "\n",
    "## Removing noise with the Fourier Transform\n",
    "It is often the case that we need to study the underlying process that drives a particular time series. To do that, we may want to remove the noise of the time series and analyze the signal.\n",
    "\n",
    "The Fourier Transform can help us achieve this objective. By moving our time series from the time domain to the frequency domain, we can filter out the frequencies that pollute the data. Then, we just have to apply the inverse Fourier transform to get a filtered version of our time series.\n",
    "\n",
    "*Note: the code presented in this section is a slightly modified version of Steven L. Brunton code. See References section to find the original code and explanation.*\n",
    "\n",
    "### 2.1. The code\n",
    "The following gist contains the necessary code to remove the noise using the Fourier Transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_denoiser(x, n_components, to_real=True):\n",
    "    \"\"\"Fast fourier transform denoiser.\n",
    "    \n",
    "    Denoises data using the fast fourier transform.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.array\n",
    "        The data to denoise.\n",
    "    n_components : int\n",
    "        The value above which the coefficients will be kept.\n",
    "    to_real : bool, optional, default: True\n",
    "        Whether to remove the complex part (True) or not (False)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    clean_data : numpy.array\n",
    "        The denoised data.\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Steve Brunton - Denoising Data with FFT[Python]\n",
    "       https://www.youtube.com/watch?v=s2K1JfNR7Sc&ab_channel=SteveBrunton\n",
    "    \n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    \n",
    "    # compute the fft\n",
    "    fft = np.fft.fft(x, n)\n",
    "    \n",
    "    # compute power spectrum density\n",
    "    # squared magnitud of each fft coefficient\n",
    "    PSD = fft * np.conj(fft) / n\n",
    "    \n",
    "    # keep high frequencies\n",
    "    _mask = PSD > n_components\n",
    "    fft = _mask * fft\n",
    "    \n",
    "    # inverse fourier transform\n",
    "    clean_data = np.fft.ifft(fft)\n",
    "    \n",
    "    if to_real:\n",
    "        clean_data = clean_data.real\n",
    "    \n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage example\n",
    "![](img/5tips_ts/fourier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing noise with the Kalman Filter\n",
    "With the Fourier Transform we obtain the frequencies that exist in a given time series, but we do not have any information of when these frequencies occur in time. This means that, in its basic form, the Fourier Transform is not the best choice for non-stationary time series.\n",
    "\n",
    "For example, financial time series are considered non-stationary (although any attempt [to prove it statistically is doomed](https://towardsdatascience.com/non-stationarity-and-memory-in-financial-markets-fcef1fe76053)), thus making Fourier a bad choice.\n",
    "\n",
    "At this point, we can choose to apply the Fourier Transform in a rolling-basis or to go with a [Wavelet Transform](http://users.rowan.edu/~polikar/WTpart1.html). But there is a much more interesting algorithm **called Kalman Filter**.\n",
    "\n",
    "The [Kalman Filter](https://www.cs.unc.edu/~welch/media/pdf/kalman_intro.pdf) is essentially a [Bayesian Linear Regression](https://users.aalto.fi/~ssarkka/pub/cup_book_online_20131111.pdf) that can optimally estimate the hidden state of a process using its observable variables.\n",
    "\n",
    "By carefully selecting the right parameters, one can tweak the algorithm to extract the underlying signal.\n",
    "\n",
    "### 3.1. The code\n",
    "[I created a small library](https://github.com/Xylambda/kalmanfilter) that contains a univariate Kalman Filter that can be used to extract the signal. In the README you will find the particular set of parameters I used. You can also use [PyKalman](https://pykalman.github.io/).\n",
    "\n",
    "### 3.2. Example\n",
    "![](img/5tips_ts/kalman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Outliers\n",
    "Outliers are usually undesirable because they deeply affect our conclusions if we are not careful when dealing with them. For example, the Pearson correlation formula can have a very different result if there are large enough outliers in our data.\n",
    "\n",
    "Outlier analysis and filtering in time series requires a more sophisticated approach than in normal data, since **you cannot use future information to filter past outliers.**\n",
    "\n",
    "One quick way to remove outliers is doing it in a rolling/expanding basis. A common algorithm to find outliers is computing the mean and standard deviation of our data and check which values are n standard deviations above or below the mean (typically, n is set to 3). Those values are then marked as outliers.\n",
    "\n",
    "### The code\n",
    "The following code allows you to filter outliers using the aforementioned algorithm but in rolling or expanding mode to avoid [look-ahead bias](https://corporatefinanceinstitute.com/resources/knowledge/finance/look-ahead-bias/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_filter(data, mode='rolling', window=262, threshold=3):\n",
    "    \"\"\"Basic Filter.\n",
    "    \n",
    "    Mark as outliers the points that are out of the interval:\n",
    "    (mean - threshold * std, mean + threshold * std ).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.Series\n",
    "        The time series to filter.\n",
    "    mode : str, optional, default: 'rolling'\n",
    "        Whether to filter in rolling or expanding basis.\n",
    "    window : int, optional, default: 262\n",
    "        The number of periods to compute the mean and standard\n",
    "        deviation.\n",
    "    threshold : int, optional, default: 3\n",
    "        The number of standard deviations above the mean.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    series : pandas.DataFrame\n",
    "        Original series and marked outliers.\n",
    "    \"\"\"\n",
    "    msg = f\"Type must be of pandas.Series but {type(data)} was passed.\"\n",
    "    assert isinstance(data, pd.Series), msg\n",
    "    \n",
    "    series = data.copy()\n",
    "    \n",
    "    # rolling/expanding objects\n",
    "    pd_object = getattr(series, mode)(window=window)\n",
    "    mean = pd_object.mean()\n",
    "    std = pd_object.std()\n",
    "    \n",
    "    upper_bound = mean + threshold * std\n",
    "    lower_bound = mean - threshold * std\n",
    "    \n",
    "    outliers = ~series.between(lower_bound, upper_bound)\n",
    "    # fill false positives with 0\n",
    "    outliers.iloc[:window] = np.zeros(shape=window)\n",
    "    \n",
    "    series = series.to_frame()\n",
    "    series['outliers'] = np.array(outliers.astype('int').values)\n",
    "    series.columns = ['Close', 'Outliers']\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Playing with the parameters you can fine-tune your analysis. Here is an example using the default values of the function.\n",
    "\n",
    "![](img/5tips_ts/outliers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The right way to normalize time series data.\n",
    "Many posts use the classical fit-transform approach with time series as if they could be treated as normal data. As with outliers, you cannot use future information to normalize data from the past unless you are 100% sure the values you are using to normalize are constant over time.\n",
    "\n",
    "The right way to normalize time series is in a rolling/expanding basis.\n",
    "\n",
    "### The code\n",
    "I used Sklearn API to create a class that allows you to normalize data avoiding look-ahead bias. Because it inherits BaseEstimator and TransformerMixin it is possible to embed this class in a Sklearn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class RollingStandardScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Rolling standard Scaler\n",
    "    \n",
    "    Standardized the given data series using the mean and std \n",
    "    commputed in rolling or expanding mode.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    window : int\n",
    "        Number of periods to compute the mean and std.\n",
    "    mode : str, optional, default: 'rolling'\n",
    "        Mode \n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    pd_object : pandas.Rolling\n",
    "        Pandas window object.\n",
    "    w_mean : pandas.Series\n",
    "        Series of mean values.\n",
    "    w_std : pandas.Series\n",
    "        Series of std. values.\n",
    "    \"\"\"\n",
    "    def __init__(self, window, mode='rolling'):\n",
    "        self.window = window\n",
    "        self.mode = mode\n",
    "        \n",
    "        # to fill in code\n",
    "        self.pd_object = None\n",
    "        self.w_mean = None\n",
    "        self.w_std = None\n",
    "        self.__fitted__ = False\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"RollingStandardScaler(window={self.window}, mode={self.mode})\"\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits.\n",
    "        \n",
    "        Computes the mean and std to be used for later scaling.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_shape, n_features)\n",
    "            The data used to compute the per-feature mean and std. Used for\n",
    "            later scaling along the feature axis.\n",
    "        y\n",
    "            Ignored.\n",
    "        \"\"\"\n",
    "        self.pd_object = getattr(X, self.mode)(self.window)\n",
    "        self.w_mean = self.pd_object.mean()\n",
    "        self.w_std = self.pd_object.std()\n",
    "        self.__fitted__ = True\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transforms.\n",
    "        \n",
    "        Scale features of X according to the window mean and standard \n",
    "        deviation.\n",
    "        \n",
    "        Paramaters\n",
    "        ----------\n",
    "        X : array-like of shape (n_shape, n_features)\n",
    "            Input data that will be transformed.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        standardized : array-like of shape (n_shape, n_features)\n",
    "            Transformed data.\n",
    "        \"\"\"\n",
    "        self._check_fitted()\n",
    "        \n",
    "        standardized = X.copy()\n",
    "        return (standardized - self.w_mean) / self.w_std\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"Inverse transform\n",
    "        \n",
    "        Undo the transform operation\n",
    "        \n",
    "        Paramaters\n",
    "        ----------\n",
    "        X : array-like of shape (n_shape, n_features)\n",
    "            Input data that will be transformed.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        standardized : array-like of shape (n_shape, n_features)\n",
    "            Transformed (original) data.\n",
    "        \"\"\"\n",
    "        self._check_fitted()\n",
    "        \n",
    "        unstandardized = X.copy()\n",
    "        return  (unstandardized * self.w_std) + self.w_mean\n",
    "        \n",
    "    def _check_fitted(self):\n",
    "        \"\"\" Checks if the algorithm is fitted. \"\"\"\n",
    "        if not self.__fitted__:\n",
    "            raise ValueError(\"Please, fit the algorithm first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "![](img/5tips_ts/normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A flexible way to compute returns.\n",
    "The last tip is focused on quantitative analysis of financial time series. Working with returns is the first thing you learn as a quant researcher. Hence, it is necessary to have a basic framework to quickly compute log and arithmetic returns in different periods of time.\n",
    "\n",
    "Also, when filtering financial time series, the ideal procedure filters returns first and then goes back to prices. So you are free to add this step to the code from section 4.\n",
    "\n",
    "### The code\n",
    "The following gist contains a basic framework to compute returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Utils functions. \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_returns(data, periods=1, log=False, relative=True):\n",
    "    \"\"\"Computes returns.\n",
    "\n",
    "    Calculates the returns of a given dataframe for the given period. The \n",
    "    returns can be computed as log returns or as arithmetic returns\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame or pandas.Series\n",
    "        The data to calculate returns of.\n",
    "    periods : int\n",
    "        The period difference to compute returns.\n",
    "    log : bool, optional, default: False\n",
    "        Whether to compute log returns (True) or not (False).\n",
    "    relative : bool, optional, default: True\n",
    "        Whether to compute relative returns (True) or not \n",
    "        (False).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ret : pandas.DataFrame or pandas.Series\n",
    "        The computed returns.\n",
    "    \n",
    "    \"\"\"\n",
    "    if log:\n",
    "        if not relative:\n",
    "            raise ValueError(\"Log returns are relative by definition.\")\n",
    "        else:\n",
    "            ret = _log_returns(data, periods)\n",
    "    else:\n",
    "        ret = _arithmetic_returns(data, periods, relative)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _arithmetic_returns(data, periods, relative):\n",
    "    \"\"\"Arithmetic returns.\"\"\"\n",
    "    # to avoid computing it twice\n",
    "    shifted = data.shift(periods)\n",
    "    ret = (data - shifted) \n",
    "    \n",
    "    if relative:\n",
    "        return ret / shifted\n",
    "    else:\n",
    "        return ret\n",
    "\n",
    "\n",
    "def _log_returns(data, periods):\n",
    "    \"\"\"Log returns.\"\"\"\n",
    "    return np.log(data / data.shift(periods))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "![](img/5tips_ts/returns.png)\n",
    "\n",
    "These are some of the tips I find more useful in my day-to-day basis. I really hope you find something interesting in this post and, if you find any error or would like to discuss any concept, please leave a comment and I will answer as soon as possible.\n",
    "\n",
    "## References\n",
    "1. Callum Ballard — [Making Matplotlib Beautiful By Default](https://towardsdatascience.com/making-matplotlib-beautiful-by-default-d0d41e3534fd).\n",
    "2. Steven L. Brunton — [Denoising Data with FFT [Python]](https://www.youtube.com/watch?v=s2K1JfNR7Sc&ab_channel=SteveBrunton).\n",
    "3. Greg Welch, Gary Bishop — [An introduction to the Kalman Filter](https://www.cs.unc.edu/~welch/media/pdf/kalman_intro.pdf).\n",
    "4. Simo Särkkä — Bayesian filtering and smoothing.\n",
    "5. Yves-Laurent Kom Samo — [Stationarity and Memory in Financial Markets](https://towardsdatascience.com/non-stationarity-and-memory-in-financial-markets-fcef1fe76053).\n",
    "6. Robi Polikar — [The Wavelet Tutorial](http://users.rowan.edu/~polikar/WTtutorial.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
