{
  
    
        "post0": {
            "title": "Using Python to construct dollar indices",
            "content": "# dependencies we are going to need import investpy import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from sklearn.decomposition import PCA from functools import reduce from typing import Dict, Union . Introduction . In order to track the strength of the dollar, different organizations have developed different techniques to create their own dollar indexes. In this post we are going to see 2 methodologies to construct dollar indexes plus an extra one with PCA, though we will not discuss the results (at least in this post). . The most common way of constructing an index is by weighting the currency to track against a set of ponderated currencies. The currencies basket, the way weights are chosen and how we aggregate the different ponderated currencies define how we construct an index. . USDIDX . The dollar index is an index that measures the U.S. dollar value. It is built using a geometric mean of the following currencies with their respective weights: . Euro (EUR): 57.6 % weight | Japanese yen (JPY): 13.6 % weight. | Pound sterling (GBP): 11.9 % weight. | Canadian dollar (CAD): 11.9 % weight. | Swedish krona (SEK): 4.2 % weight. | Swiss franc (CHF): 3.6 % weight. | . In the case of the main U.S. dollar index the weights remain static, which does not allow the index to capture well some market conditions. Furthermore, we see the Swedish Krona taking a relatively high weight, which is certainly outdated in the current market context. . Trade weighted U.S. dollar index . In 1998, the staff of the Federal Reserve Board introduced a new set of indices, among which was the trade weighted dollar index. . Although the dollar index is not well suited to capture rapid market movements (among other things), the trade weighted U.S. dollar index is not designed to fix this issue but to measure the strenght of the U.S. economy with respect to its trading partners. . According to Loretan [1], the Federal Reserve Board&#39;s nominal dollar index at time $t$ can be defined as $$ I_{t} = I_{t-1} cdot prod_{j=1}^{N(t)} left( dfrac{e_{j,t}}{e_{j,t-1}} right)^{w_{j,t}} $$ . Let&#39;s break the formula to understand each one of the terms: . $I_{t-1}$ is the value of the index at time $t-1$. | $e_{j,t}$ and $e_{j,t-1}$ are the prices of the U.S. dollar in terms of foreign currency $j$ at times $t$ and $t-1$. | $w_{j,t}$ is the weight of currency $j$ at time $t$. | $N(t)$ is the number of foreign currencies in the index at time $t$. | $ sum_{j}w_{j,t} = 1$. | . Notice how the index is computed using a geometric weighted average. As Loretan explains in 1, this is done because geometric averaging forces proportionately equal appreciation and depreciation of currencies to have the same numerical effect. . The weights $w_{j,t}$ are computed as . $$ w_{j,t} = frac{1}{2} mu_{US,j,t} + frac{1}{2} left( frac{1}{2} epsilon_{US, j,t} + frac{1}{2} tau_{US,jt} right) $$which is nothing more than a linear combination of three submeasures of the degree of trade competition. . In the formula above, we can identify some terms that need further explanation: . $ mu_{US,j,t} = M_{US,j,t} / sum_{j=1}^{N(t)}M_{US, j,t}$ is the economy&#39;s bilateral import weight during period $t$. Here, $M_{US,j,t}$ represents the merchandise imports from economy $j$ to the USA in year $t$. . | $ epsilon_{US, j,t} = X_{US,j,t} / sum_{j=1}^{N(t)}X_{US,j,t}$ accounts for the US bilateral export share, where $X_{US,j,t}$ represents the merchandise exports from the USA to economy $j$ in year $t$. . | $ tau_{US,j,t} = sum_{k neq j, k neq US}^{N(t)} epsilon_{US, j,t} cdot mu_{k,j,t} / (1 - mu_{j,j,t})$ measures a form of competition where US-produced goods may also compete with goods produced in economy $j$ if the USA and economy $j$ both export goods to buyers in third-market economies. Here, $ mu_{k,j,t}$ is the fraction of economy $k$&#39;s merchandise imports from country $j$ in year $t$. The factor $1 / (1 - mu_{j,j,t})$ is used to ensure weights sum up to 1. . | . Fortunately for us, we don&#39;t have to manually compute the weights since they can be found at the Federal Reserve Board [4]. . Constructing dollar indices in Python . US dollar index . We first start with the US dollar index. The construction is pretty simple since it is just a geometric weighted mean. The formula to compute the index is: $$ USDIDX = 50.14348112 cdot EURUSD^{-0.576} cdot USDJPY^{0.136} cdot GBPUSD^{-0.119} cdot USDCAD^{0.091} cdot USDSEK^{0.042} cdot USDCHF^{0.036} $$ . Notice how the sign of the exponent changes to account for the direction in which the currencies are expressed: if the USD is the base currency, the exponent is positive. . The number 50.14348112 is a correction factor to force the index to start at value 100. . #collapse-show def dollar_index( currencies: Dict[str, pd.DataFrame], weights: Dict[str, float] ) -&gt; Union[pd.DataFrame, pd.Series]: &quot;&quot;&quot;Compute the main U.S. dollar index. Weights must account for the currency cross direction. Parameters currencies : dict Dictionary containing the currency prices. weights : dict Dictionary containing the weights of each currency. Returns - idx : pandas.DataFrame U.S. dollar index. &quot;&quot;&quot; new = {} idx_crosses = [&#39;EUR/USD&#39;, &#39;USD/JPY&#39;, &#39;GBP/USD&#39;, &#39;USD/CAD&#39;, &#39;USD/SEK&#39;, &#39;USD/CHF&#39;] # ponderate each currency for key in idx_crosses: new[key] = currencies[key] ** weights[key] # multiply all currencies idx = reduce( lambda a, b: a.multiply(b), [new[key] for key in new.keys()] ) # add correction factor idx *= 50.14348112 return idx . . # create weights dictionary idx_crosses = [&#39;EUR/USD&#39;, &#39;USD/JPY&#39;, &#39;GBP/USD&#39;, &#39;USD/CAD&#39;, &#39;USD/SEK&#39;, &#39;USD/CHF&#39;] dollar_idx_weights = dict(zip(idx_crosses, [-0.576, 0.136, -0.119, 0.091, 0.042, 0.036])) # compute the us dollar index usdidx = dollar_index(crosses_dict, dollar_idx_weights) usdidx[&#39;Close&#39;].plot(figsize=(15,7), title=&#39;USD IDX&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;USD IDX&#39;}, xlabel=&#39;Date&#39;&gt; . findfont: Font family [&#39;Franklin Gothic Book&#39;] not found. Falling back to DejaVu Sans. findfont: Font family [&#39;Franklin Gothic Book&#39;] not found. Falling back to DejaVu Sans. . Trade weighted dollar index . In order to get the weights quickly and in a simple way, we can just copy the table from [4] and use Pandas to read our clipboard. . #collapse-show weights = pd.read_clipboard() weights.set_index(&#39;Country or Region&#39;, inplace=True) weights.index = crosses + [&#39;Total&#39;] # put currency names instead of country names . . weights . 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 2008 2007 2006 . AUD/USD 1.401 | 1.401 | 1.401 | 1.416 | 1.453 | 1.443 | 1.539 | 1.557 | 1.596 | 1.749 | 1.674 | 1.549 | 1.639 | 1.480 | 1.364 | 1.309 | . USD/ARS 0.442 | 0.442 | 0.442 | 0.495 | 0.550 | 0.524 | 0.510 | 0.479 | 0.530 | 0.507 | 0.513 | 0.469 | 0.447 | 0.456 | 0.380 | 0.359 | . USD/BRL 1.932 | 1.932 | 1.932 | 1.952 | 2.010 | 1.903 | 2.080 | 2.338 | 2.426 | 2.428 | 2.448 | 2.194 | 2.051 | 2.114 | 1.857 | 1.779 | . USD/CAD 13.337 | 13.337 | 13.337 | 13.467 | 13.685 | 13.873 | 14.062 | 15.120 | 15.513 | 15.645 | 15.883 | 16.078 | 15.844 | 17.406 | 18.089 | 18.613 | . USD/CNY 13.719 | 13.719 | 13.719 | 15.767 | 16.014 | 15.635 | 15.862 | 15.645 | 15.564 | 15.099 | 14.798 | 14.848 | 14.035 | 13.009 | 12.839 | 12.326 | . USD/CLP 0.640 | 0.640 | 0.640 | 0.642 | 0.633 | 0.621 | 0.651 | 0.637 | 0.706 | 0.731 | 0.701 | 0.606 | 0.611 | 0.610 | 0.586 | 0.600 | . USD/COP 0.615 | 0.615 | 0.615 | 0.624 | 0.598 | 0.610 | 0.653 | 0.704 | 0.661 | 0.670 | 0.659 | 0.650 | 0.678 | 0.666 | 0.591 | 0.552 | . USD/HKD 1.330 | 1.330 | 1.330 | 1.438 | 1.489 | 1.433 | 1.420 | 1.450 | 1.418 | 1.314 | 1.349 | 1.317 | 1.332 | 1.233 | 1.246 | 1.239 | . USD/IDR 0.665 | 0.665 | 0.665 | 0.669 | 0.678 | 0.668 | 0.698 | 0.726 | 0.762 | 0.737 | 0.770 | 0.747 | 0.699 | 0.679 | 0.616 | 0.597 | . USD/INR 2.869 | 2.869 | 2.869 | 2.800 | 2.674 | 2.627 | 2.458 | 2.310 | 2.264 | 2.228 | 2.220 | 2.120 | 2.069 | 1.917 | 1.746 | 1.499 | . USD/ILS 0.985 | 0.985 | 0.985 | 1.037 | 1.051 | 1.122 | 1.149 | 1.138 | 1.145 | 1.154 | 1.229 | 1.183 | 1.219 | 1.257 | 1.209 | 1.168 | . USD/JPY 6.377 | 6.377 | 6.377 | 6.282 | 6.383 | 6.498 | 6.359 | 6.681 | 7.072 | 7.568 | 7.191 | 7.501 | 7.263 | 7.931 | 8.340 | 9.065 | . USD/KRW 3.283 | 3.283 | 3.283 | 3.273 | 3.325 | 3.319 | 3.400 | 3.347 | 3.333 | 3.264 | 3.329 | 3.278 | 3.044 | 2.937 | 2.961 | 3.076 | . USD/MYR 1.278 | 1.278 | 1.278 | 1.232 | 1.261 | 1.294 | 1.229 | 1.170 | 1.127 | 1.115 | 1.198 | 1.310 | 1.310 | 1.402 | 1.472 | 1.752 | . USD/MXN 13.698 | 13.698 | 13.698 | 13.452 | 13.189 | 13.341 | 13.331 | 12.867 | 12.610 | 12.261 | 11.787 | 11.604 | 10.988 | 10.686 | 10.899 | 11.281 | . USD/PHP 0.662 | 0.662 | 0.662 | 0.644 | 0.642 | 0.625 | 0.601 | 0.610 | 0.608 | 0.626 | 0.613 | 0.619 | 0.615 | 0.651 | 0.672 | 0.717 | . USD/RUB 0.468 | 0.468 | 0.468 | 0.514 | 0.523 | 0.462 | 0.509 | 0.699 | 0.697 | 0.692 | 0.674 | 0.591 | 0.636 | 0.761 | 0.647 | 0.635 | . USD/SAR 0.511 | 0.511 | 0.511 | 0.482 | 0.562 | 0.641 | 0.694 | 0.649 | 0.732 | 0.719 | 0.592 | 0.559 | 0.664 | 0.564 | 0.529 | 0.423 | . USD/SEK 0.559 | 0.559 | 0.559 | 0.549 | 0.541 | 0.543 | 0.554 | 0.576 | 0.560 | 0.607 | 0.658 | 0.709 | 0.770 | 0.774 | 0.794 | 0.829 | . USD/SGD 1.891 | 1.891 | 1.891 | 1.870 | 1.681 | 1.613 | 1.569 | 1.447 | 1.517 | 1.696 | 1.719 | 1.757 | 1.692 | 1.585 | 1.685 | 1.698 | . USD/CHF 2.844 | 2.844 | 2.844 | 2.632 | 2.752 | 2.599 | 2.452 | 2.400 | 2.383 | 2.295 | 2.227 | 2.362 | 2.504 | 2.166 | 1.920 | 1.823 | . USD/TWD 2.145 | 2.145 | 2.145 | 1.940 | 1.941 | 2.018 | 2.010 | 2.024 | 2.008 | 2.073 | 2.285 | 2.322 | 2.055 | 2.176 | 2.358 | 2.469 | . USD/THB 1.088 | 1.088 | 1.088 | 1.052 | 1.065 | 1.075 | 1.053 | 1.023 | 1.030 | 1.025 | 1.014 | 1.043 | 1.019 | 1.049 | 1.063 | 1.121 | . GBP/USD 5.416 | 5.416 | 5.416 | 5.449 | 5.291 | 5.406 | 5.526 | 5.287 | 5.107 | 5.272 | 5.338 | 5.511 | 6.082 | 6.023 | 6.150 | 6.002 | . USD/VND 1.761 | 1.761 | 1.761 | 1.350 | 1.322 | 1.339 | 1.148 | 0.934 | 0.806 | 0.703 | 0.642 | 0.609 | 0.593 | 0.491 | 0.413 | 0.333 | . EUR/USD 20.086 | 20.086 | 20.086 | 18.972 | 18.685 | 18.769 | 18.481 | 18.182 | 17.824 | 17.822 | 18.493 | 18.464 | 20.141 | 19.976 | 19.575 | 18.735 | . Total 100.000 | 100.000 | 100.000 | 100.000 | 100.000 | 100.000 | 100.000 | 100.000 | 100.000 | 100.000 | 100.000 | 100.000 | 100.000 | 100.000 | 100.000 | 100.000 | . #collapse-show def trade_weighted_usdidx(currencies: Dict[str, pd.DataFrame], weights: pd.DataFrame) -&gt; pd.DataFrame: &quot;&quot;&quot; Compute trade weighted dollar index. Parameters - currencies : dict Dictionary containing all currency crosses. weights : pandas.DataFrame DataFrame containing the weights for each cross. The columns should be years while the index should be the crosses. Returns - idx : pd.DataFrame Trade weighted dollar index. &quot;&quot;&quot; ponderated_crosses = {} for cross in currencies.keys(): ponderations_per_year = [] for year in weights.columns: # invert prices if needed if not cross.startswith(&#39;USD&#39;): prices = (1 / currencies[cross]).loc[str(year)] else: prices = currencies[cross].loc[str(year)] returns = prices / prices.shift(1) weight = weights.loc[cross, year] / 100 ponderations_per_year.append(returns ** weight) ponderated_crosses[cross] = pd.concat(ponderations_per_year) # multiply all currencies idx = reduce( lambda a, b: a.multiply(b), [ponderated_crosses[key] for key in ponderated_crosses.keys()] ) idx.iloc[0, :] = 100 return idx.cumprod() . . Since we could not retrieve the USD/SAR prices for the year 2021, we will only compute the index from 2006 to 2020. . fed_idx = trade_weighted_usdidx(currencies=crosses_dict, weights=weights.drop(&#39;2021&#39;, axis=1)) fed_idx.plot(figsize=(15,7), title=&#39;Trade weighted dollar index&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Trade weighted dollar index&#39;}, xlabel=&#39;Date&#39;&gt; . Build your own index . Note: this idea was inspired by [2] and [6]. . In this section, we will code a simple index using PCA to compute the weights of the currencies. The idea is to reduce the space into 1 component and then grab the loadings (projected eigenvalues) to use them as weights. . The ideal procedure will compute PCA using a rolling window, but I am gonna leave that as an exercise for the reader ;). . #collapse closes = [] for cr in crosses_dict.keys(): closes.append(crosses_dict[cr].loc[:, &#39;Close&#39;]) closes = pd.concat(closes, axis=1) closes.columns = crosses_dict.keys() . . We will apply PCA over the logarithmic daily returns. . returns = closes.apply(np.log).diff(1) pca = PCA(n_components=1) pca.fit(returns.dropna()) . PCA(n_components=1) . Now, Scikit-Learn stores an eigenvector, for each principal component, for the projection space. We can use these values as our weights. Below there is a plot showing the values for these weights. . weights_pca = pd.Series(index=returns.columns, data=pca.components_[0]) weights_pca.plot.bar(figsize=(15,7), title=&#39;Weights&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Weights&#39;}&gt; . Notice how PCA is able to capture the sign of the returns: if the direction is XXX/USD, the weights are negative. Another thing to take into account is the fact that, generally, the crosses with the lowest weights are pegged currencies (USD/CNY, USD/HKD, USD/SAR, USD/VND). . Going back to the procedure, since the PCA weights do not sum up to 1, we have to normalize them but taking into account the sign: . weights_pca = (weights_pca / weights_pca.abs().sum()) weights_pca.plot.bar(figsize=(15,7), title=&#39;Fixed Weights&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Fixed Weights&#39;}&gt; . #collapse-show def pca_dollar_index( currencies: Dict[str, pd.DataFrame], weights: Dict[str, float] ) -&gt; Union[pd.DataFrame, pd.Series]: &quot;&quot;&quot;Compute dollar index using PCA. Weights must account for the currency cross direction. Parameters currencies : dict Dictionary containing the currency prices. weights : dict Dictionary containing the weights of each currency. Returns - idx : pandas.DataFrame U.S. dollar index. &quot;&quot;&quot; new = {} # ponderate each currency for key in idx_crosses: new[key] = currencies[key] ** weights[key] # multiply all currencies idx = reduce( lambda a, b: a.multiply(b), [new[key] for key in new.keys()] ) norm_factor = 100 / idx[&#39;Close&#39;].dropna().iloc[0] idx *= norm_factor return idx . . The operation norm_factor = 100 / idx[&#39;Close&#39;].dropna().iloc[0] is used to force the first day to be 100, as in the other indices. . pca_index = pca_dollar_index(crosses_dict, weights_pca) pca_index.plot(figsize=(15,7), title=&#39;PCA Index&#39;) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;PCA Index&#39;}, xlabel=&#39;Date&#39;&gt; . Putting all together we can see how the indices evolve: . #collapse all_indices = pd.concat( [usdidx[&#39;Close&#39;], fed_idx[&#39;Close&#39;], pca_index[&#39;Close&#39;]], axis=1 ) all_indices.columns = [&#39;USD IDX&#39;, &#39;FED IDX&#39;, &#39;PCA IDx&#39;] all_indices.plot(figsize=(15,7)) . . &lt;AxesSubplot:xlabel=&#39;Date&#39;&gt; . And their correlations . #collapse-hide plt.figure(figsize=(14,8)) sns.heatmap(all_indices.corr(), annot=True, annot_kws={&#39;fontsize&#39;:20}) . . &lt;AxesSubplot:&gt; . findfont: Font family [&#39;Franklin Gothic Book&#39;] not found. Falling back to DejaVu Sans. . As expected, all indices are highly correlated, which means they have a strong linear codependence. . Conclusions . In this post we&#39;ve reviewed 3 ways of computing a dollar index: . USDIDX: the most common dollar index. . | Trade weighted dollar index: an index designed by the USA Federal Reserve to measure how well is USA performing in relation to its trading parters. . | PCA index: based on [2] and [6], which is an index whose weights have been computed depending on the eigenvalues of the covariance matrix of the closing prices returns. . | . The construction of the indices is not perfect in the sense that they don&#39;t replicate the true values of the indices. This is due to the lack of data on some days plus a difference in the prices caused by the subset of the market that was used to get the prices not being the same (probably). . An additional analysis will have to be carried out to check which is index is better, but that is out of the scope of this post. You can check [7] to have an idea of how to evaluate the goodness of an index, though it will depend on the use case. . References . [1] Mico Loretan - Indexes of the Foreign Exchange Value of the Dollar . | [2] Musa Essayyad, Khaled Albinali and Omar Al-Titi - Constructing an alternative dollar index to gauge the movements in currency markets . | [3] The Ice - U.S. Dollar Index Contracts . | [4] Federal Reserve Board - Total Trade Weights . | [5] S&amp;P Dow Jones Indices: Index Methodology . | [6] Yao Lei Xu - Stock Market Analytics with PCA. . | [7] Cerno Capital - Is the US Dollar Index Fit for Purpose . | .",
            "url": "https://xylambda.github.io/blog/python/finance/2021/07/29/dollar-index.html",
            "relUrl": "/python/finance/2021/07/29/dollar-index.html",
            "date": " • Jul 29, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Building a Trainer class for PyTorch models",
            "content": "The training process . Let&#39;s start by refreshing the algorithm for training a neural network. . We first compute the forward pass of the model. | Then, we calculate the loss using whatever function we have decided to use. | Once we have the loss, we backpropagate the error through the model. | Finally, using the errors, we update the parameters. | We keep repeating these four steps until convergence. . What is wrong with PyTorch? . Nothing really, you just have to build the training loop by calling specific built-in routines in the right order, unlike in Keras where you just have to call fit method to optimize your models. That is why it can be a little confusing and the beginning and definitely time-consuming. . What are we going to build? . We are going to create a class called Trainer that is going to imitate the behaviour of Keras training process. That is, we just need call fit to optimize our models while we drink a cup of coffe. . I personally want the trainer to have the following features: . Train the model passing the loaders, loss and optimizer. | Control the train and validation loss. | Easy to read. | Extendable. For example, easy to add new features like L1 or ElasticNet regularization. | . A note on Dataset and DataLoader . Dataset and DataLoader are the default classes to feed a model in PyTorch efficiently. Basically, Dataset wrapps your data and DataLoader loads the data into the model. . I recommed you reading this article from Standford University if you are unfamiliar with the topic. . We will work assuming our dataset is designed following the next code: . import torch from torch.utils.data import Dataset class DataWrapper(Dataset): &quot;&quot;&quot; Class to wrap a dataset. Assumes X and y are already torch tensors and have the right data type and shape. Parameters - X : torch.Tensor Features tensor. y : torch.Tensor Labels tensor. &quot;&quot;&quot; def __init__(self, X, y): self.features = X self.labels = y def __len__(self): return len(self.features) def __getitem__(self, idx): return self.features[idx], self.labels[idx] . The code . The fit structure is . class Trainer: ... def fit(self, train_loader, val_loader, epochs): for epoch in range(epochs): # train train_loss = self._train(train_loader) # validate val_loss = self._validate(val_loader) . Now we need to create the _train and _validate functions. The train implements the optimization algorithm we saw at the beginning of the post. The validation is not used to optimize, so we do not need to compute the gradients. Fortunatelly, PyTorch provides with a primitive to prevent this to happen: torch.no_grad() . class Trainer: ... def _train(self, loader): # put model in train mode self.model.train() for features, labels in loader: # forward pass out = self.model(features) # loss loss = self._compute_loss(out, labels) # remove gradient from previous passes self.optimizer.zero_grad() # backprop loss.backward() # parameters update self.optimizer.step() return loss.item() def _validate(self, loader): # put model in evaluation mode self.model.eval() with torch.no_grad(): for features, labels in loader: out = self.model(features) loss = self._compute_loss(out, labels) return loss.item() . We return loss.item() to get the loss value and not the entire graph. . Lastly, I created a separate function to compute loss because: . Sometimes, the target needs to be casted to long dtype and I wanted the Trainer the handle it. | You can add regularization penalties in this function. | . class Trainer: ... def _compute_loss(self, real, target): try: loss = self.criterion(real, target) except: loss = self.criterion(real, target.long()) msg = f&quot;Target tensor has been casted from&quot; msg = f&quot;{msg} {type(target)} to &#39;long&#39; dtype to avoid errors.&quot; warnings.warn(msg) . The complete code for the trainer: . #collapse import time import logging import warnings class Trainer: &quot;&quot;&quot;Trainer Class that eases the training of a PyTorch model. Parameters - model : torch.Module The model to train. criterion : torch.Module Loss function criterion. optimizer : torch.optim Optimizer to perform the parameters update. logger_kwards : dict Args for .. Attributes - train_loss_ : list val_loss_ : list &quot;&quot;&quot; def __init__( self, model, criterion, optimizer, logger_kwargs, device=None ): self.model = model self.criterion = criterion self.optimizer = optimizer self.logger_kwargs = logger_kwargs self.device = self._get_device(device) # send model to device self.model.to(self.device) # attributes self.train_loss_ = [] self.val_loss_ = [] logging.basicConfig(level=logging.INFO) def fit(self, train_loader, val_loader, epochs): &quot;&quot;&quot;Fits. Fit the model using the given loaders for the given number of epochs. Parameters - train_loader : val_loader : epochs : int Number of training epochs. &quot;&quot;&quot; # track total training time total_start_time = time.time() # - train process - for epoch in range(epochs): # track epoch time epoch_start_time = time.time() # train tr_loss = self._train(train_loader) # validate val_loss = self._validate(val_loader) self.train_loss_.append(tr_loss) self.val_loss_.append(val_loss) epoch_time = time.time() - epoch_start_time self._logger( tr_loss, val_loss, epoch+1, epochs, epoch_time, **self.logger_kwargs ) total_time = time.time() - total_start_time # final message logging.info( f&quot;&quot;&quot;End of training. Total time: {round(total_time, 5)} seconds&quot;&quot;&quot; ) def _logger( self, tr_loss, val_loss, epoch, epochs, epoch_time, show=True, update_step=20 ): if show: if epoch % update_step == 0 or epoch == 1: # to satisfy pep8 common limit of characters msg = f&quot;Epoch {epoch}/{epochs} | Train loss: {tr_loss}&quot; msg = f&quot;{msg} | Validation loss: {val_loss}&quot; msg = f&quot;{msg} | Time/epoch: {round(epoch_time, 5)} seconds&quot; logging.info(msg) def _train(self, loader): self.model.train() for features, labels in loader: # move to device features, labels = self._to_device(features, labels, self.device) # forward pass out = self.model(features) # loss loss = self._compute_loss(out, labels) # remove gradient from previous passes self.optimizer.zero_grad() # backprop loss.backward() # parameters update self.optimizer.step() return loss.item() def _to_device(self, features, labels, device): return features.to(device), labels.to(device) def _validate(self, loader): self.model.eval() with torch.no_grad(): for features, labels in loader: # move to device features, labels = self._to_device( features, labels, self.device ) out = self.model(features) loss = self._compute_loss(out, labels) return loss.item() def _compute_loss(self, real, target): try: loss = self.criterion(real, target) except: loss = self.criterion(real, target.long()) msg = f&quot;Target tensor has been casted from&quot; msg = f&quot;{msg} {type(target)} to &#39;long&#39; dtype to avoid errors.&quot; warnings.warn(msg) # apply regularization if any # loss += penalty.item() return loss def _get_device(self, device): if device is None: dev = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) msg = f&quot;Device was automatically selected: {dev}&quot; warnings.warn(msg) else: dev = device return dev . . I&#39;ve added a logger and code to send the model and data to a given device. . Example of use . Using Sklearn, we are going to generate dummy data and train a simple regression model. . #collapse-show import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split np.random.seed(2) torch.manual_seed(0) X, y = make_regression( n_samples=10000, n_features=1, n_informative=1, n_targets=1, noise=10 ) plt.plot(X, y, &quot;.&quot;) . . [&lt;matplotlib.lines.Line2D at 0x21067668be0&gt;] . #collapse # to avoid errors y = y.reshape(-1,1) # two splits to get train, test and validation datasets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33) X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.33) # transform to torch tensor X_train, X_test, X_val = torch.from_numpy(X_train).float(), torch.from_numpy(X_test).float(), torch.from_numpy(X_val).float() y_train, y_test, y_val = torch.from_numpy(y_train).float(), torch.from_numpy(y_test).float(), torch.from_numpy(y_val).float() . . A simple neuron will be enough to create a linear regression model . #collapse-show import torch.nn as nn import torch.optim as optim model = nn.Linear(in_features=1, out_features=1) criterion = nn.MSELoss() optimizer = optim.Adam(model.parameters()) . . Now we have to wrap our data and create the DataLoaders to pass to our trainer: . #collapse-show from torch.utils.data import DataLoader train_loader = DataLoader(DataWrapper(X_train, y_train), batch_size=32) val_loader = DataLoader(DataWrapper(X_val, y_val), batch_size=32) . . Finally, we train our model . #collapse-show device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) logger_kwars = {&#39;update_step&#39;: 20, &#39;show&#39;: True} trainer = Trainer( model=model, criterion=criterion, optimizer=optimizer, logger_kwargs=logger_kwars, device=device ) trainer.fit(train_loader, val_loader, 120) . . INFO:root:Epoch 1/120 | Train loss: 614.618896484375 | Validation loss: 159.5471649169922 | Time/epoch: 0.56739 seconds INFO:root:Epoch 20/120 | Train loss: 511.74383544921875 | Validation loss: 137.21205139160156 | Time/epoch: 0.40326 seconds INFO:root:Epoch 40/120 | Train loss: 415.39007568359375 | Validation loss: 118.22764587402344 | Time/epoch: 0.36988 seconds INFO:root:Epoch 60/120 | Train loss: 331.61614990234375 | Validation loss: 107.15283203125 | Time/epoch: 0.35907 seconds INFO:root:Epoch 80/120 | Train loss: 261.1725769042969 | Validation loss: 105.73088836669922 | Time/epoch: 0.36289 seconds INFO:root:Epoch 100/120 | Train loss: 204.3862762451172 | Validation loss: 114.3382797241211 | Time/epoch: 0.36488 seconds INFO:root:Epoch 120/120 | Train loss: 161.37001037597656 | Validation loss: 132.109375 | Time/epoch: 0.36012 seconds INFO:root:End of training. Total time: 44.73981 seconds . We can see overfitting in the last epochs. Let&#39;s ignore that and continue with the predictions . #collapse-show with torch.no_grad(): y_pred = model(X_test.to(device)).cpu().numpy() plt.plot(X_test.numpy(), y_test.numpy(), &quot;.&quot;, label=&#39;y test&#39;) plt.plot(X_test.numpy(), y_pred, &quot;.&quot;, label=&#39;Predicion&#39;) plt.legend(); . . The trainer seems to work perfectly. . Can we improve it? . Of course. It is a very primitive class right now. We can add functions to handle regulatization, lr scheduling, early stopping, etc. You are free to use this code for your own purposes. . Conclusion . In this post we&#39;ve seen how to create a really simple trainer class to optimize PyTorch models. From this design, we can go further and add functionality to make it more complete. I&#39;ve created a library from this basic design that includes, amon other things, a callbacks system that allows to interact with the model during the training process. . PyTorch is great framework, but it can be a little intimidating for DL newcomers. Although there are many great tools out there, being able to create your own training loop will help you better understand the basics of the framework. . Thank you for reading! . References . Afshine Amidi and Shervine Amidi - A detailed example of how to generate your data in parallel with PyTorch |",
            "url": "https://xylambda.github.io/blog/python/pytorch/machine-learning/2021/01/04/pytorch_trainer.html",
            "relUrl": "/python/pytorch/machine-learning/2021/01/04/pytorch_trainer.html",
            "date": " • Jan 4, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "PyTorch vs TensorFlow vs NumPy",
            "content": "Introduction . When it comes to Deep Learning I consider PyTorch as my default framework. It is easy to use, fast, elegant and debugging is pretty intuitive. The alternative the market offers is TensorFlow, which combined with Keras provides a powerful tool to create complex models. . On the other hand, in my day-to-day basis I use NumPy, a well-known library to create and manipulate arrays that supports other libraries (such as Pandas or Scikit-Learn). . Although NumPy does not fall in the Deep Learning frameworks category, I think it is natural to ask which one of these frameworks performs better as a tool to manipulate Tensors. . In this post, I try to cast some light by benchmarking the speed of these three libraries. . The current versions I&#39;m using for this experiment are . !pip freeze | grep &quot;torch |numpy |tensorflow&quot; . numpy==1.18.5 tensorflow==2.3.1 tensorflow-estimator==2.3.0 torch==1.7.0 . The specs of my PC: . !sysctl -n machdep.cpu.brand_string . Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz . mem = !sysctl -n hw.memsize f&quot;{int(mem[0]) / (1024**3)} GB&quot; . &#39;16.0 GB&#39; . Lastly, I am running TensorFlow in eager mode to make my life (much more) easier. . tf.executing_eagerly() . True . The experiments . The procedure is simple. Test a set of operations for different sizes on the three libraries. Store the results and study them at the end. . The experiments are: . Creation: study time required to create tensor-like structures. | Sorting: analyze time to sort a tensor-like variable. | Matrix multiplication: check time to multiply to tensors. | Searching/Traversing: test time required to perform a linear search (traverse the structure). | Creation . In this section, the speed for creating array-like structures will be tested for the three libraries. . # sizes to study SIZES = [1000000, 50000000, 100000000, 200000000, 500000000] . #collapse np_creation = np.zeros(len(SIZES)) torch_creation = np.zeros(len(SIZES)) tf_creation = np.zeros(len(SIZES)) for i, size in enumerate(SIZES): start = time.time() tf.random.uniform(shape=(size,)) end = time.time() tf_creation[i] = end - start # numpy for i, size in enumerate(SIZES): start = time.time() np.random.rand(size) end = time.time() np_creation[i] = end - start # torch for i, size in enumerate(SIZES): start = time.time() torch.rand(size) end = time.time() torch_creation[i] = end - start . . Sorting . Sorting is one of the most difficult tasks for a processor. Moving elements from register to register takes a lot of time. This makes it a perfect candidate to test the speed of a particular framework. . #collapse np_sort = np.zeros(len(SIZES)) torch_sort = np.zeros(len(SIZES)) tf_sort = np.zeros(len(SIZES)) for i, size in enumerate(SIZES): _to_sort = tf.random.uniform(shape=(size,)) start = time.time() tf.sort(_to_sort, axis=-1, direction=&quot;ASCENDING&quot;) end = time.time() tf_sort[i] = end - start # numpy for i, size in enumerate(SIZES): _to_sort = np.random.rand(size) start = time.time() np.sort(_to_sort) end = time.time() np_sort[i] = end - start # torch for i, size in enumerate(SIZES): _to_sort = torch.rand(size) start = time.time() torch.sort(_to_sort, descending=False) end = time.time() torch_sort[i] = end - start . . Matrix multiplication . A naive procedure for multiplying two matrices can take up to $O(n^3)$ steps. Let&#39;s test how fast these frameworks have become in this aspect using their default matrix multiplication function (no Einsten summation or similar stuff). . #collapse np_matmul = np.zeros(len(SIZES)) torch_matmul = np.zeros(len(SIZES)) tf_matmul = np.zeros(len(SIZES)) for i, size in enumerate(SIZES): _a = tf.random.uniform(shape=(1,size)) _b = tf.reshape(_a, shape=(size,1)) start = time.time() tf.linalg.matmul(_a, _b) end = time.time() tf_matmul[i] = end - start # numpy for i, size in enumerate(SIZES): _a = np.random.rand(size) _b = _a.reshape(-1,1) start = time.time() np.matmul(_a, _b) end = time.time() np_matmul[i] = end - start # torch for i, size in enumerate(SIZES): _a = torch.rand(size) _b = _a.reshape(-1,1) start = time.time() torch.matmul(_a, _b) end = time.time() torch_matmul[i] = end - start . . Searching . We will perform a linear search with a simple loop. In fact, this test will also assess the traverse speed of the different frameworks. . I have coded the solution using a for loop because I&#39;ve already know that the element to search is in the last position; hence, we can consider this loop as a counter loop where the number of iterations is known beforehand. . Furthermore, I have explicitly add a conditional statement to the loop to be slightly more intensive. . # the original sizes take too long SIZES_ = [10000, 50000, 100000, 500000, 1000000] . It seems that, generally, TensorFlow objects are not assignable. That&#39;s why I used NumPy, then tf.Variable and lastly, tf.convert_to_tensor to get a tensor with the desired values. . #collapse-show np_search = np.zeros(len(SIZES_)) torch_search = np.zeros(len(SIZES_)) tf_search = np.zeros(len(SIZES_)) for i, size in enumerate(SIZES_): _np = np.zeros(size) _np[-1] = 27 _to_search = tf.convert_to_tensor(tf.Variable(_np)) start = time.time() for j, element in enumerate(_to_search): if element == 27: end = time.time() tf_search[i] = end - start # numpy for i, size in enumerate(SIZES_): _to_search = np.zeros(size) _to_search[-1] = 27 start = time.time() for j, element in enumerate(_to_search): if element == 27: end = time.time() np_search[i] = end - start # torch for i, size in enumerate(SIZES_): _to_search = torch.zeros(size) _to_search[-1] = 27 start = time.time() for j, element in enumerate(_to_search): if element == 27: end = time.time() torch_search[i] = end - start . . Results . #collapse fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(17,12)) ax[0][0].plot(SIZES, np_creation, label=&quot;Numpy&quot;) ax[0][0].plot(SIZES, torch_creation, label=&quot;PyTorch&quot;) ax[0][0].plot(SIZES, tf_creation, label=&quot;TensorFlow&quot;) ax[0][0].set_title(&quot;Creation&quot;) ax[0][1].plot(SIZES, np_sort, label=&quot;Numpy&quot;) ax[0][1].plot(SIZES, torch_sort, label=&quot;PyTorch&quot;) ax[0][1].plot(SIZES, tf_sort, label=&quot;TensorFlow&quot;) ax[0][1].set_title(&quot;Sorting&quot;) ax[1][0].plot(SIZES_, np_search, label=&quot;Numpy&quot;) ax[1][0].plot(SIZES_, torch_search, label=&quot;PyTorch&quot;) ax[1][0].plot(SIZES_, tf_search, label=&quot;TensorFlow&quot;) ax[1][0].set_title(&quot;Searching&quot;) ax[1][1].plot(SIZES, np_matmul, label=&quot;Numpy&quot;) ax[1][1].plot(SIZES, torch_matmul, label=&quot;PyTorch&quot;) ax[1][1].plot(SIZES, tf_matmul, label=&quot;TensorFlow&quot;) ax[1][1].set_title(&quot;Matmul&quot;) lines, labels = fig.axes[-1].get_legend_handles_labels() fig.legend(lines, labels, loc = &#39;center&#39;, ncol=3, bbox_to_anchor=[0.4, 0.04], fontsize=23); . . WARNING: Logging before flag parsing goes to stderr. W1208 22:13:05.004575 4497706496 font_manager.py:1282] findfont: Font family [&#39;Franklin Gothic Book&#39;] not found. Falling back to DejaVu Sans. W1208 22:13:05.025423 4497706496 font_manager.py:1282] findfont: Font family [&#39;Franklin Gothic Book&#39;] not found. Falling back to DejaVu Sans. W1208 22:13:05.172847 4497706496 font_manager.py:1282] findfont: Font family [&#39;Franklin Gothic Book&#39;] not found. Falling back to DejaVu Sans. . I am surprised by how well NumPy performs in general. It is faster in everything except in the creation test. One possible explanation is that NumPy is more optimized for the CPU. Unfortunatelly, I do not have a GPU to test this and, even if I had one, NumPy does not support GPU execution natively. . Regarding PyTorch and Tensorflow, I do not see a clear winner. Searching and sorting takes an insane amount of time for Tensorflow compared with the others, but I am running it in eager mode to make the code human-readible. . Which one should I use? . There is no clear answer to that question because it depends on what you want to achieve. If you work in a Data Science field with Python, you would probably need to use NumPy even if you don&#39;t want to (though I can&#39;t image why you wouldn&#39;t want to). . In terms of Deep Learning research, I think PyTorch is more well-suited than TensorFlow because it is easier to learn and to iterate over the models. . Regarding Production-level code, I would consider TensorFlow (with eager mode deactivated) the best one. It is one of the oldest and a lot of services support TensorFlow integration. . Conclusion . It seems that NumPy is a very good candidate to set up your data science stack. In terms of Deep Learning, it will depend on your objectives: Tensorflow for performance vs PyTorch for ease of development, . Personally, I will still be using PyTorch as my main Deep Learning tool since I work in the research stage and not at production level, and NumPy as my array-manipulation library. .",
            "url": "https://xylambda.github.io/blog/python/numpy/pytorch/tensorflow/2020/12/08/torch-vs-numpy-vs-tf.html",
            "relUrl": "/python/numpy/pytorch/tensorflow/2020/12/08/torch-vs-numpy-vs-tf.html",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "5 tips for working with time series in Python",
            "content": "This is a repost from my medium article 5 tips for working with time series in Python . . Required knowledge . This post is pretty easy to follow if you already have some basic knowledge of Pandas, NumPy and Python. I will not go into much details with the theoretical stuff but use the resources at the end or ask a question if you need clarification about some particular concept. . Removing noise with the Fourier Transform . It is often the case that we need to study the underlying process that drives a particular time series. To do that, we may want to remove the noise of the time series and analyze the signal. . The Fourier Transform can help us achieve this objective. By moving our time series from the time domain to the frequency domain, we can filter out the frequencies that pollute the data. Then, we just have to apply the inverse Fourier transform to get a filtered version of our time series. . Note: the code presented in this section is a slightly modified version of Steven L. Brunton code. See References section to find the original code and explanation. . &#160;2.1. The code . The following gist contains the necessary code to remove the noise using the Fourier Transform: . def fft_denoiser(x, n_components, to_real=True): &quot;&quot;&quot;Fast fourier transform denoiser. Denoises data using the fast fourier transform. Parameters - x : numpy.array The data to denoise. n_components : int The value above which the coefficients will be kept. to_real : bool, optional, default: True Whether to remove the complex part (True) or not (False) Returns - clean_data : numpy.array The denoised data. References - .. [1] Steve Brunton - Denoising Data with FFT[Python] https://www.youtube.com/watch?v=s2K1JfNR7Sc&amp;ab_channel=SteveBrunton &quot;&quot;&quot; n = len(x) # compute the fft fft = np.fft.fft(x, n) # compute power spectrum density # squared magnitud of each fft coefficient PSD = fft * np.conj(fft) / n # keep high frequencies _mask = PSD &gt; n_components fft = _mask * fft # inverse fourier transform clean_data = np.fft.ifft(fft) if to_real: clean_data = clean_data.real return clean_data . Usage example . Removing noise with the Kalman Filter . With the Fourier Transform we obtain the frequencies that exist in a given time series, but we do not have any information of when these frequencies occur in time. This means that, in its basic form, the Fourier Transform is not the best choice for non-stationary time series. . For example, financial time series are considered non-stationary (although any attempt to prove it statistically is doomed), thus making Fourier a bad choice. . At this point, we can choose to apply the Fourier Transform in a rolling-basis or to go with a Wavelet Transform. But there is a much more interesting algorithm called Kalman Filter. . The Kalman Filter is essentially a Bayesian Linear Regression that can optimally estimate the hidden state of a process using its observable variables. . By carefully selecting the right parameters, one can tweak the algorithm to extract the underlying signal. . 3.1. The code . I created a small library that contains a univariate Kalman Filter that can be used to extract the signal. In the README you will find the particular set of parameters I used. You can also use PyKalman. . 3.2. Example . Dealing with Outliers . Outliers are usually undesirable because they deeply affect our conclusions if we are not careful when dealing with them. For example, the Pearson correlation formula can have a very different result if there are large enough outliers in our data. . Outlier analysis and filtering in time series requires a more sophisticated approach than in normal data, since you cannot use future information to filter past outliers. . One quick way to remove outliers is doing it in a rolling/expanding basis. A common algorithm to find outliers is computing the mean and standard deviation of our data and check which values are n standard deviations above or below the mean (typically, n is set to 3). Those values are then marked as outliers. . The code . The following code allows you to filter outliers using the aforementioned algorithm but in rolling or expanding mode to avoid look-ahead bias. . def basic_filter(data, mode=&#39;rolling&#39;, window=262, threshold=3): &quot;&quot;&quot;Basic Filter. Mark as outliers the points that are out of the interval: (mean - threshold * std, mean + threshold * std ). Parameters - data : pandas.Series The time series to filter. mode : str, optional, default: &#39;rolling&#39; Whether to filter in rolling or expanding basis. window : int, optional, default: 262 The number of periods to compute the mean and standard deviation. threshold : int, optional, default: 3 The number of standard deviations above the mean. Returns - series : pandas.DataFrame Original series and marked outliers. &quot;&quot;&quot; msg = f&quot;Type must be of pandas.Series but {type(data)} was passed.&quot; assert isinstance(data, pd.Series), msg series = data.copy() # rolling/expanding objects pd_object = getattr(series, mode)(window=window) mean = pd_object.mean() std = pd_object.std() upper_bound = mean + threshold * std lower_bound = mean - threshold * std outliers = ~series.between(lower_bound, upper_bound) # fill false positives with 0 outliers.iloc[:window] = np.zeros(shape=window) series = series.to_frame() series[&#39;outliers&#39;] = np.array(outliers.astype(&#39;int&#39;).values) series.columns = [&#39;Close&#39;, &#39;Outliers&#39;] return series . Example . Playing with the parameters you can fine-tune your analysis. Here is an example using the default values of the function. . . The right way to normalize time series data. . Many posts use the classical fit-transform approach with time series as if they could be treated as normal data. As with outliers, you cannot use future information to normalize data from the past unless you are 100% sure the values you are using to normalize are constant over time. . The right way to normalize time series is in a rolling/expanding basis. . The code . I used Sklearn API to create a class that allows you to normalize data avoiding look-ahead bias. Because it inherits BaseEstimator and TransformerMixin it is possible to embed this class in a Sklearn pipeline. . from sklearn.base import BaseEstimator, TransformerMixin class RollingStandardScaler(BaseEstimator, TransformerMixin): &quot;&quot;&quot;Rolling standard Scaler Standardized the given data series using the mean and std commputed in rolling or expanding mode. Parameters - window : int Number of periods to compute the mean and std. mode : str, optional, default: &#39;rolling&#39; Mode Attributes - pd_object : pandas.Rolling Pandas window object. w_mean : pandas.Series Series of mean values. w_std : pandas.Series Series of std. values. &quot;&quot;&quot; def __init__(self, window, mode=&#39;rolling&#39;): self.window = window self.mode = mode # to fill in code self.pd_object = None self.w_mean = None self.w_std = None self.__fitted__ = False def __repr__(self): return f&quot;RollingStandardScaler(window={self.window}, mode={self.mode})&quot; def fit(self, X, y=None): &quot;&quot;&quot;Fits. Computes the mean and std to be used for later scaling. Parameters - X : array-like of shape (n_shape, n_features) The data used to compute the per-feature mean and std. Used for later scaling along the feature axis. y Ignored. &quot;&quot;&quot; self.pd_object = getattr(X, self.mode)(self.window) self.w_mean = self.pd_object.mean() self.w_std = self.pd_object.std() self.__fitted__ = True return self def transform(self, X): &quot;&quot;&quot;Transforms. Scale features of X according to the window mean and standard deviation. Paramaters - X : array-like of shape (n_shape, n_features) Input data that will be transformed. Returns - standardized : array-like of shape (n_shape, n_features) Transformed data. &quot;&quot;&quot; self._check_fitted() standardized = X.copy() return (standardized - self.w_mean) / self.w_std def inverse_transform(self, X): &quot;&quot;&quot;Inverse transform Undo the transform operation Paramaters - X : array-like of shape (n_shape, n_features) Input data that will be transformed. Returns - standardized : array-like of shape (n_shape, n_features) Transformed (original) data. &quot;&quot;&quot; self._check_fitted() unstandardized = X.copy() return (unstandardized * self.w_std) + self.w_mean def _check_fitted(self): &quot;&quot;&quot; Checks if the algorithm is fitted. &quot;&quot;&quot; if not self.__fitted__: raise ValueError(&quot;Please, fit the algorithm first.&quot;) . Example . A flexible way to compute returns. . The last tip is focused on quantitative analysis of financial time series. Working with returns is the first thing you learn as a quant researcher. Hence, it is necessary to have a basic framework to quickly compute log and arithmetic returns in different periods of time. . Also, when filtering financial time series, the ideal procedure filters returns first and then goes back to prices. So you are free to add this step to the code from section 4. . The code . The following gist contains a basic framework to compute returns . &quot;&quot;&quot; Utils functions. &quot;&quot;&quot; import numpy as np import pandas as pd def compute_returns(data, periods=1, log=False, relative=True): &quot;&quot;&quot;Computes returns. Calculates the returns of a given dataframe for the given period. The returns can be computed as log returns or as arithmetic returns Parameters - data : pandas.DataFrame or pandas.Series The data to calculate returns of. periods : int The period difference to compute returns. log : bool, optional, default: False Whether to compute log returns (True) or not (False). relative : bool, optional, default: True Whether to compute relative returns (True) or not (False). Returns - ret : pandas.DataFrame or pandas.Series The computed returns. &quot;&quot;&quot; if log: if not relative: raise ValueError(&quot;Log returns are relative by definition.&quot;) else: ret = _log_returns(data, periods) else: ret = _arithmetic_returns(data, periods, relative) return ret def _arithmetic_returns(data, periods, relative): &quot;&quot;&quot;Arithmetic returns.&quot;&quot;&quot; # to avoid computing it twice shifted = data.shift(periods) ret = (data - shifted) if relative: return ret / shifted else: return ret def _log_returns(data, periods): &quot;&quot;&quot;Log returns.&quot;&quot;&quot; return np.log(data / data.shift(periods)) . Example . These are some of the tips I find more useful in my day-to-day basis. I really hope you find something interesting in this post and, if you find any error or would like to discuss any concept, please leave a comment and I will answer as soon as possible. . References . Callum Ballard — Making Matplotlib Beautiful By Default. | Steven L. Brunton — Denoising Data with FFT [Python]. | Greg Welch, Gary Bishop — An introduction to the Kalman Filter. | Simo Särkkä — Bayesian filtering and smoothing. | Yves-Laurent Kom Samo — Stationarity and Memory in Financial Markets. | Robi Polikar — The Wavelet Tutorial. |",
            "url": "https://xylambda.github.io/blog/python/numpy/pandas/2020/12/06/ts_tips.html",
            "relUrl": "/python/numpy/pandas/2020/12/06/ts_tips.html",
            "date": " • Dec 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Who are you? . Hi! I’m Alejandro, a Data Scientist. I am currently working as quantitative researcher, developing mathematical models to understand the FX market. . What did you study? . My academic background includes a Bachelor of Science in Computer Science and a Big Data minor. The latter as a result of my stay abroad in Netherlands. In addition, I am currently studying a Master’s degree in Artificial Intelligence. . I want to know you . I like films and music. I mean, I REALLY like films and music. I play piano and guitar in my free time and I recorded some homemade films (that will never see the light) with my friends. . Besides, I like to read fantasy books and travelling and I’m also interested in politics and science in general. . I want to contact you . You can contact me using the email I have on my GitHub web profile. . How did you make this site? . This site is built with fastpages, an easy to use blogging platform with extra features for Jupyter Notebooks. .",
          "url": "https://xylambda.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://xylambda.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}