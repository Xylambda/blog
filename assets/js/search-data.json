{
  
    
        "post0": {
            "title": "",
            "content": "This is a repost from my medium article 5 tips for working with time series in Python . . At some point in his/her career, any Data Scientist has to be able to manipulate time series data. I have been working as a Data Scientist and Quant Researcher for the last 14 months and I found little “cooking tips” for working with this type of data. Today, I would like to share some of those tips. . Required knowledge . This post is pretty easy to follow if you already have some basic knowledge of Pandas, NumPy and Python. I will not go into much details with the theoretical stuff but use the resources at the end or ask a question if you need clarification about some particular concept. . Removing noise with the Fourier Transform . It is often the case that we need to study the underlying process that drives a particular time series. To do that, we may want to remove the noise of the time series and analyze the signal. . The Fourier Transform can help us achieve this objective. By moving our time series from the time domain to the frequency domain, we can filter out the frequencies that pollute the data. Then, we just have to apply the inverse Fourier transform to get a filtered version of our time series. . Note: the code presented in this section is a slightly modified version of Steven L. Brunton code. See References section to find the original code and explanation. . &#160;2.1. The code . The following gist contains the necessary code to remove the noise using the Fourier Transform: . def fft_denoiser(x, n_components, to_real=True): &quot;&quot;&quot;Fast fourier transform denoiser. Denoises data using the fast fourier transform. Parameters - x : numpy.array The data to denoise. n_components : int The value above which the coefficients will be kept. to_real : bool, optional, default: True Whether to remove the complex part (True) or not (False) Returns - clean_data : numpy.array The denoised data. References - .. [1] Steve Brunton - Denoising Data with FFT[Python] https://www.youtube.com/watch?v=s2K1JfNR7Sc&amp;ab_channel=SteveBrunton &quot;&quot;&quot; n = len(x) # compute the fft fft = np.fft.fft(x, n) # compute power spectrum density # squared magnitud of each fft coefficient PSD = fft * np.conj(fft) / n # keep high frequencies _mask = PSD &gt; n_components fft = _mask * fft # inverse fourier transform clean_data = np.fft.ifft(fft) if to_real: clean_data = clean_data.real return clean_data . Usage example . Removing noise with the Kalman Filter . With the Fourier Transform we obtain the frequencies that exist in a given time series, but we do not have any information of when these frequencies occur in time. This means that, in its basic form, the Fourier Transform is not the best choice for non-stationary time series. . For example, financial time series are considered non-stationary (although any attempt to prove it statistically is doomed), thus making Fourier a bad choice. . At this point, we can choose to apply the Fourier Transform in a rolling-basis or to go with a Wavelet Transform. But there is a much more interesting algorithm called Kalman Filter. . The Kalman Filter is essentially a Bayesian Linear Regression that can optimally estimate the hidden state of a process using its observable variables. . By carefully selecting the right parameters, one can tweak the algorithm to extract the underlying signal. . 3.1. The code . I created a small library that contains a univariate Kalman Filter that can be used to extract the signal. In the README you will find the particular set of parameters I used. You can also use PyKalman. . 3.2. Example . Dealing with Outliers . Outliers are usually undesirable because they deeply affect our conclusions if we are not careful when dealing with them. For example, the Pearson correlation formula can have a very different result if there are large enough outliers in our data. . Outlier analysis and filtering in time series requires a more sophisticated approach than in normal data, since you cannot use future information to filter past outliers. . One quick way to remove outliers is doing it in a rolling/expanding basis. A common algorithm to find outliers is computing the mean and standard deviation of our data and check which values are n standard deviations above or below the mean (typically, n is set to 3). Those values are then marked as outliers. . The code . The following code allows you to filter outliers using the aforementioned algorithm but in rolling or expanding mode to avoid look-ahead bias. . def basic_filter(data, mode=&#39;rolling&#39;, window=262, threshold=3): &quot;&quot;&quot;Basic Filter. Mark as outliers the points that are out of the interval: (mean - threshold * std, mean + threshold * std ). Parameters - data : pandas.Series The time series to filter. mode : str, optional, default: &#39;rolling&#39; Whether to filter in rolling or expanding basis. window : int, optional, default: 262 The number of periods to compute the mean and standard deviation. threshold : int, optional, default: 3 The number of standard deviations above the mean. Returns - series : pandas.DataFrame Original series and marked outliers. &quot;&quot;&quot; msg = f&quot;Type must be of pandas.Series but {type(data)} was passed.&quot; assert isinstance(data, pd.Series), msg series = data.copy() # rolling/expanding objects pd_object = getattr(series, mode)(window=window) mean = pd_object.mean() std = pd_object.std() upper_bound = mean + threshold * std lower_bound = mean - threshold * std outliers = ~series.between(lower_bound, upper_bound) # fill false positives with 0 outliers.iloc[:window] = np.zeros(shape=window) series = series.to_frame() series[&#39;outliers&#39;] = np.array(outliers.astype(&#39;int&#39;).values) series.columns = [&#39;Close&#39;, &#39;Outliers&#39;] return series . Example . Playing with the parameters you can fine-tune your analysis. Here is an example using the default values of the function. . . The right way to normalize time series data. . Many posts use the classical fit-transform approach with time series as if they could be treated as normal data. As with outliers, you cannot use future information to normalize data from the past unless you are 100% sure the values you are using to normalize are constant over time. . The right way to normalize time series is in a rolling/expanding basis. . The code . I used Sklearn API to create a class that allows you to normalize data avoiding look-ahead bias. Because it inherits BaseEstimator and TransformerMixin it is possible to embed this class in a Sklearn pipeline. . from sklearn.base import BaseEstimator, TransformerMixin class RollingStandardScaler(BaseEstimator, TransformerMixin): &quot;&quot;&quot;Rolling standard Scaler Standardized the given data series using the mean and std commputed in rolling or expanding mode. Parameters - window : int Number of periods to compute the mean and std. mode : str, optional, default: &#39;rolling&#39; Mode Attributes - pd_object : pandas.Rolling Pandas window object. w_mean : pandas.Series Series of mean values. w_std : pandas.Series Series of std. values. &quot;&quot;&quot; def __init__(self, window, mode=&#39;rolling&#39;): self.window = window self.mode = mode # to fill in code self.pd_object = None self.w_mean = None self.w_std = None self.__fitted__ = False def __repr__(self): return f&quot;RollingStandardScaler(window={self.window}, mode={self.mode})&quot; def fit(self, X, y=None): &quot;&quot;&quot;Fits. Computes the mean and std to be used for later scaling. Parameters - X : array-like of shape (n_shape, n_features) The data used to compute the per-feature mean and std. Used for later scaling along the feature axis. y Ignored. &quot;&quot;&quot; self.pd_object = getattr(X, self.mode)(self.window) self.w_mean = self.pd_object.mean() self.w_std = self.pd_object.std() self.__fitted__ = True return self def transform(self, X): &quot;&quot;&quot;Transforms. Scale features of X according to the window mean and standard deviation. Paramaters - X : array-like of shape (n_shape, n_features) Input data that will be transformed. Returns - standardized : array-like of shape (n_shape, n_features) Transformed data. &quot;&quot;&quot; self._check_fitted() standardized = X.copy() return (standardized - self.w_mean) / self.w_std def inverse_transform(self, X): &quot;&quot;&quot;Inverse transform Undo the transform operation Paramaters - X : array-like of shape (n_shape, n_features) Input data that will be transformed. Returns - standardized : array-like of shape (n_shape, n_features) Transformed (original) data. &quot;&quot;&quot; self._check_fitted() unstandardized = X.copy() return (unstandardized * self.w_std) + self.w_mean def _check_fitted(self): &quot;&quot;&quot; Checks if the algorithm is fitted. &quot;&quot;&quot; if not self.__fitted__: raise ValueError(&quot;Please, fit the algorithm first.&quot;) . Example . A flexible way to compute returns. . The last tip is focused on quantitative analysis of financial time series. Working with returns is the first thing you learn as a quant researcher. Hence, it is necessary to have a basic framework to quickly compute log and arithmetic returns in different periods of time. . Also, when filtering financial time series, the ideal procedure filters returns first and then goes back to prices. So you are free to add this step to the code from section 4. . The code . The following gist contains a basic framework to compute returns . &quot;&quot;&quot; Utils functions. &quot;&quot;&quot; import numpy as np import pandas as pd def compute_returns(data, periods=1, log=False, relative=True): &quot;&quot;&quot;Computes returns. Calculates the returns of a given dataframe for the given period. The returns can be computed as log returns or as arithmetic returns Parameters - data : pandas.DataFrame or pandas.Series The data to calculate returns of. periods : int The period difference to compute returns. log : bool, optional, default: False Whether to compute log returns (True) or not (False). relative : bool, optional, default: True Whether to compute relative returns (True) or not (False). Returns - ret : pandas.DataFrame or pandas.Series The computed returns. &quot;&quot;&quot; if log: if not relative: raise ValueError(&quot;Log returns are relative by definition.&quot;) else: ret = _log_returns(data, periods) else: ret = _arithmetic_returns(data, periods, relative) return ret def _arithmetic_returns(data, periods, relative): &quot;&quot;&quot;Arithmetic returns.&quot;&quot;&quot; # to avoid computing it twice shifted = data.shift(periods) ret = (data - shifted) if relative: return ret / shifted else: return ret def _log_returns(data, periods): &quot;&quot;&quot;Log returns.&quot;&quot;&quot; return np.log(data / data.shift(periods)) . Example . These are some of the tips I find more useful in my day-to-day basis. I really hope you find something interesting in this post and, if you find any error or would like to discuss any concept, please leave a comment and I will answer as soon as possible. . References . Callum Ballard — Making Matplotlib Beautiful By Default. | Steven L. Brunton — Denoising Data with FFT [Python]. | Greg Welch, Gary Bishop — An introduction to the Kalman Filter. | Simo Särkkä — Bayesian filtering and smoothing. | Yves-Laurent Kom Samo — Stationarity and Memory in Financial Markets. | Robi Polikar — The Wavelet Tutorial. |",
            "url": "https://xylambda.github.io/blog/2020/12/06/2020-12-06-tips-time-series.html",
            "relUrl": "/2020/12/06/2020-12-06-tips-time-series.html",
            "date": " • Dec 6, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Factor Analysis from scratch with NumPy and Pandas",
            "content": "In my job, in the most recent project I was envolved into, I had to deal with hidden variables. Searching on the web I found a paper that had to solve a similar issue, and they chose to use factor analysis. . I honestly knew nothing about factor analysis, so I started to search and read information from different sources. I found that most resources were too opaque, while others didn&#39;t explain the concepts in detail and relied its content on explaining the extraction process using libraries such psych or programs like SPSS. . This post aims to solve these problems by explaining the theory (I should say aggregating theory from different sources listed at the end) and then implementing things from scratch. Let&#39;s begin! . 1. Theory . 1.1. Definitions . Factor analysis is a multivariate method that aims to represent $y_{p}$ observable variables as a linear combination of $f_{m}$ factors, with $m &lt; p$. The factor analysis model can be expressed as follows 1 2: begin{matrix} y_{1} - mu_{1} = a_{11} f_{1} + a_{12} f_{2} + ... + a_{1m} f_{m} + epsilon_{1} y_{2} - mu_{2} = a_{11} f_{1} + a_{12} f_{2} + ... + a_{1m} f_{m} + epsilon_{1} vdots y_{p} - mu_{p} = a_{p1} f_{1} + a_{p2} f_{2} + ... + a_{pm} f_{m} + epsilon_{p} end{matrix} . where $ mu$ is the mean vector, $a_{ij}$ are the loadings and $ epsilon$ is the error vector. The loadings are nothing more than regression weights for the factors. They can be viewed as the contributions of each factor in estimating the original variables. . Why don&#39;t we use a simple regression in this case? The answer is pretty obvious: we cannot observe the independent variables, which are the hidden factors. . The errors $ epsilon$ serve to indicate that the hypothesized relationships are not exact 3. That is, we try to explain the variance of our variables, but we assume we cannot explain all the variance. These errors account for this lack of precision. . We can write the factor analysis model in matrix notation: begin{equation} Y - mu = A cdot F + varepsilon end{equation} . 1.2. Assumptions . The first assumtion is that the existance of these hidden variables is an hypothesis; that is, we cannot assume that these variables really exist. . Other assumptions are listed below 1 2: . $E[F] = 0$ | $cov[F] = E[FF^{T}] = I$, remember $I$ is the identity matrix. | $cov[ epsilon_{i}, epsilon_{j}] = 0$, no association between errors. | $E[ varepsilon] = 0$ | $cov[ varepsilon] = Psi $, where | . $$ Psi = left( begin{matrix} Psi_{1} &amp; 0 &amp; ... &amp; 0 0 &amp; Psi_{2} &amp; ... &amp; 0 0 &amp; 0 &amp; ... &amp; Psi_{p} end{matrix} right) $$ $cov(F, varepsilon) = 0$, they are independent. | . 1.3. Factor extraction. . In this post we are only going to see one type of extraction method: principal component. If you work with data, you probably heard these 2 words before. The name for this extraction procedure may be a little misleading but it is true that FA and PCA are related. . With this procedure, we try to approximate the covariance/correlation matrix $S$ (they are equal if the data ara standardized) with this expression: $$ S cong A A^{T} + Psi $$ . $A$ is the loading matrix while $ Psi$ is the uniqueness matrix. The uniqueness accounts for the proportion of variance that is inherent to each variable and cannot be explain by the factors. . 1.4. Which number of factors should I use? . A common criterion to use when deciding the number of factors is the scree plot. . 2. Implementation . For the implementation, we will use Numpy and Pandas. NumPy will be used to handle almost all calculations while Pandas will take care of keeping the format in a nice and suited way for further processing. . import numpy as np import pandas as pd . We are going to work with the following dummy data. It is a small matrix on purpose, to make it easier to follow the steps. . # Data a = np.array([[1, 5, 5, 1, 1], [8, 9, 7, 9, 8], [9, 8, 9, 9, 8], [9, 9, 9, 9, 9], [1, 9, 1, 1, 9], [9, 7, 7, 9, 9], [9, 7, 9, 9, 7]]) # format columns = [&#39;Kind&#39;, &#39;Intelligent&#39;, &#39;Happy&#39;, &#39;Likeable&#39;, &#39;Just&#39;] index = [&#39;FSM1&#39;, &#39;Sister&#39;, &#39;FSM2&#39;, &#39;Father&#39;, &#39;Teacher&#39;, &#39;MSM&#39;, &#39;FSM3&#39;] # convert to dataframe data = pd.DataFrame(a, columns=columns, index=index) data . Kind Intelligent Happy Likeable Just . FSM1 | 1 | 5 | 5 | 1 | 1 | . Sister | 8 | 9 | 7 | 9 | 8 | . FSM2 | 9 | 8 | 9 | 9 | 8 | . Father | 9 | 9 | 9 | 9 | 9 | . Teacher | 1 | 9 | 1 | 1 | 9 | . MSM | 9 | 7 | 7 | 9 | 9 | . FSM3 | 9 | 7 | 9 | 9 | 7 | . To create our factor analysis procedure we will follow Scikit-Learn philosophy, which I find very well designed. We will create a class with 2 main public methods: fit() and transform(). The rest of the methods will serve as helpers of the main functions. . class EFA: &quot;&quot;&quot;Exploratory factor analysis. A class to perform exploratory factor analysis using the principal component method to extract the factors. Attributes - loadings : pandas.DataFrame The loadings matrix. Default to None if fit() has not been called. communalities : pandas.DataFrame The communalities matrix. Default to None if fit() has not been called. variance_summary : pandas.DataFrame The variance summary of the data. Default to None if fit() has not been called. original_data : pandas.DataFrame The original data. Deafault to None if &#39;fit()&#39; has not been called yet. Parameters - n_factors : int The number of factors to extract. is_corr : bool, optional, default: False If True, the passed data in &#39;fit()&#39; must be a correlation matrix. &quot;&quot;&quot; def __init__(self, n_factors, is_corr=False): self.n_factors = n_factors self.is_corr = is_corr self.loadings_ = None self.communalities_ = None self.variance_summary_ = None def fit(self, x): &quot;&quot;&quot;Fits. Fits the factor analysis model and computes loadings, communalities and uniquenes matrices. Parameters - x : pandas.DataFrame The data to fit the model. &quot;&quot;&quot; corr_m = self._validate_input(x) # store data self.original_data = corr_m # Principal factor extraction self._principal_factor(corr_m) def _validate_input(self, x): &quot;&quot;&quot;Validates input. Checks whether the input is appropiate or not. Parameters - x : pandas.DataFrame The input to check. &quot;&quot;&quot; if not isinstance(x, pd.DataFrame): raise ValueError(&quot;Please, pass &#39;x&#39; as pandas.DataFrame&quot;) if self.is_corr: assert len(x) == len(x.columns), &quot;Data is not symmetric&quot; corr_m = x else: corr_m = x.corr() return corr_m def transform(self, x): &quot;&quot;&quot;Transforms. &quot;&quot;&quot; pass def _create_variance_summary(self, w): &quot;&quot;&quot;Creates variances summary. Creates a DataFrame containing the variance summary of the given data. Parameters - w : numpy.array An array containing the eigenvalues. &quot;&quot;&quot; # Round to 3 decimals w = np.round(w, 3) total = w.sum() perc_var = np.array([100 * (x/total) for x in w]) aux_dict = {&quot;Eigenvalues&quot;: w, &quot;Variance %&quot;: perc_var, &quot;Cum. Variance %&quot;: perc_var.cumsum()} self.variance_summary_ = pd.DataFrame(aux_dict, index=self.original_data.index) def _principal_factor(self, corr_matrix): &quot;&quot;&quot;Principal factor procedure&quot;&quot;&quot; orig_idx = self.original_data.index # Singular value decomposition u, s, v = np.linalg.svd(corr_matrix) # compute % of each eigenvalue self._create_variance_summary(s) # Compute loadings and subset them a = pd.DataFrame(u * np.sqrt(s), index=orig_idx) self.loadings_ = a.iloc[:, 0:self.n_factors] # Compute communalities self._compute_communalities() def _compute_communalities(self): &quot;&quot;&quot;Computes communalities&quot;&quot;&quot; orig_idx = self.original_data.index # communalities computation squared_sum = (self.loadings_ ** 2).sum(axis=1) # Format dataframe self.communalities_ = pd.DataFrame(round(squared_sum, 3), index=orig_idx, columns=[&#39;Communalities&#39;]) . data.corr() . Kind Intelligent Happy Likeable Just . Kind | 1.000000 | 0.295536 | 0.880572 | 0.995429 | 0.544567 | . Intelligent | 0.295536 | 1.000000 | -0.021744 | 0.326164 | 0.837288 | . Happy | 0.880572 | -0.021744 | 1.000000 | 0.866667 | 0.130337 | . Likeable | 0.995429 | 0.326164 | 0.866667 | 1.000000 | 0.544016 | . Just | 0.544567 | 0.837288 | 0.130337 | 0.544016 | 1.000000 | . fa = EFA(n_factors=2, is_corr=False) . fa.fit(data) . fa.variance_summary_ . Eigenvalues Variance % Cum. Variance % . Kind | 3.263 | 65.26 | 65.26 | . Intelligent | 1.538 | 30.76 | 96.02 | . Happy | 0.168 | 3.36 | 99.38 | . Likeable | 0.031 | 0.62 | 100.00 | . Just | 0.000 | 0.00 | 100.00 | . fa.loadings_ . 0 1 . Kind | -0.969455 | 0.231148 | . Intelligent | -0.519402 | -0.806945 | . Happy | -0.784517 | 0.587241 | . Likeable | -0.970870 | 0.209949 | . Just | -0.703964 | -0.666927 | . fa.communalities_ . Communalities . Kind | 0.993 | . Intelligent | 0.921 | . Happy | 0.960 | . Likeable | 0.987 | . Just | 0.940 | . References . 1. C.M Cuadras - Nuevos Métodos de Análisis Multivariante. CMC Editions. Barcelona, 2010 ↩ . 2. Albin C. Rencher - Methods of Multivariate Analysis. John Wiley &amp; Sons. ↩ . 3. Peter Tryfos - Chapter 14: Factor Analysis!↩ . 4. NCSS - Chapter 420: Factor Analysis!↩ .",
            "url": "https://xylambda.github.io/blog/factor%20analysis/numpy/pandas/multivariate%20analysis/2020/04/10/factor-analysis_1.html",
            "relUrl": "/factor%20analysis/numpy/pandas/multivariate%20analysis/2020/04/10/factor-analysis_1.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://xylambda.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Who are you? . I’m Alejandro, a Data Scientist. I am currently working as quantitative researcher, developing mathematical models to understand the FX market. . What did you study? . My academic background includes a Bachelor of Science in Computer Science and a Big Data minor. The latter as a result of my stay abroad in Netherlands. . I want to know you . I like films and music. I play piano and guitar in my free time and I also like to read fantasy books. I’m also interested in politics and science in general. . I want to contact you . You can contact me using the email I have on my GitHub web profile. .",
          "url": "https://xylambda.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://xylambda.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}