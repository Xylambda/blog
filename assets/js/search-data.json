{
  
    
        "post0": {
            "title": "PyTorch vs TensorFlow vs NumPy",
            "content": "Introduction . When it comes to Deep Learning I consider PyTorch as my default framework. It is easy to use, fast, elegant and debugging is pretty intuitive. The alternative the market offers is TensorFlow, which combined with Keras provides a powerful tool to create complex models. . On the other hand, in my day-to-day basis I use NumPy, a well-known library to create and manipulate arrays that supports other libraries (such as Pandas or Scikit-Learn). . Although NumPy does not fall in the Deep Learning frameworks category, I think it is natural to ask which one of these frameworks performs better as a tool to manipulate Tensors. . In this post, I try to cast some light by benchmarking the speed of these three libraries. . #collapse import time import torch import numpy as np import pandas as pd import tensorflow as tf import matplotlib.pyplot as plt plt.style.use(&quot;ggplot&quot;) . . The current versions I&#39;m using for this experiment are . !pip freeze | grep &quot;torch |numpy |tensorflow&quot; . numpy==1.18.5 tensorflow==2.3.1 tensorflow-estimator==2.3.0 torch==1.7.0 . The specs of my PC: . !sysctl -n machdep.cpu.brand_string . Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz . mem = !sysctl -n hw.memsize f&quot;{int(mem[0]) / (1024**3)} GB&quot; . &#39;16.0 GB&#39; . Lastly, I am running TensorFlow in eager mode to make my life easier. . tf.executing_eagerly() . True . The experiments . The procedure is simple. Test a set of operations for different sizes on the three libraries. Store the results and study them at the end. . The experiments are: . Creation: study time required to create tensor-like structures. | Sorting: analyze time to sort a tensor-like variable. | Matrix multiplication: check time to multiply to tensors. | Searching/Traversing: test time required to perform a linear search (traverse the structure). | Creation . In this section, the speed for creating array-like structures will be tested for the three libraries. . # sizes to study SIZES = [1000000, 50000000, 100000000, 200000000, 500000000] . #collapse np_creation = np.zeros(len(SIZES)) torch_creation = np.zeros(len(SIZES)) tf_creation = np.zeros(len(SIZES)) . . #collapse for i, size in enumerate(SIZES): start = time.time() tf.random.uniform(shape=(size,)) end = time.time() tf_creation[i] = end - start . . #collapse for i, size in enumerate(SIZES): start = time.time() np.random.rand(size) end = time.time() np_creation[i] = end - start . . #collapse for i, size in enumerate(SIZES): start = time.time() torch.rand(size) end = time.time() torch_creation[i] = end - start . . Sorting . Sorting is one of the most difficult tasks for a processor. Moving elements from register to register takes a lot of time. This makes it a perfect candidate to test the speed of a particular framework. . #collapse np_sort = np.zeros(len(SIZES)) torch_sort = np.zeros(len(SIZES)) tf_sort = np.zeros(len(SIZES)) . . #collapse-show for i, size in enumerate(SIZES): _to_sort = tf.random.uniform(shape=(size,)) start = time.time() tf.sort(_to_sort, axis=-1, direction=&quot;ASCENDING&quot;) end = time.time() tf_sort[i] = end - start . . #collapse for i, size in enumerate(SIZES): _to_sort = np.random.rand(size) start = time.time() np.sort(_to_sort) end = time.time() np_sort[i] = end - start . . #collapse for i, size in enumerate(SIZES): _to_sort = torch.rand(size) start = time.time() torch.sort(_to_sort, descending=False) end = time.time() torch_sort[i] = end - start . . Matrix multiplication . A naive procedure for multiplying two matrices can take up to $O(n^3)$ steps. Let&#39;s test how fast these frameworks have become in this aspect. . #collapse np_matmul = np.zeros(len(SIZES)) torch_matmul = np.zeros(len(SIZES)) tf_matmul = np.zeros(len(SIZES)) . . It seems that, generally, TensorFlow objects are not assignable. That&#39;s why I used NumPy, then tf.Variable and lastly, tf.convert_to_tensor to get a tensor with the desired values. . #collapse-show for i, size in enumerate(SIZES): _a = tf.random.uniform(shape=(1,size)) _b = tf.reshape(_a, shape=(size,1)) start = time.time() tf.linalg.matmul(_a, _b) end = time.time() tf_matmul[i] = end - start . . #collapse for i, size in enumerate(SIZES): _a = np.random.rand(size) _b = _a.reshape(-1,1) start = time.time() np.matmul(_a, _b) end = time.time() np_matmul[i] = end - start . . #collapse for i, size in enumerate(SIZES): _a = torch.rand(size) _b = _a.reshape(-1,1) start = time.time() torch.matmul(_a, _b) end = time.time() torch_matmul[i] = end - start . . Searching . We will perform a linear search with a simple loop. In fact, this test will also assess the traverse speed of the different frameworks. . I have coded the solution using a for loop because I&#39;ve already know that the element to search is in the last position; hence, we can consider this loop as a counter loop where the number of iterations is known beforehand. . # the original sizes take too long SIZES_ = [10000, 50000, 100000, 500000, 1000000] . #collapse np_search = np.zeros(len(SIZES_)) torch_search = np.zeros(len(SIZES_)) tf_search = np.zeros(len(SIZES_)) . . It seems that, generally, TensorFlow objects are not assignable. That&#39;s why I used NumPy, then tf.Variable and lastly, tf.convert_to_tensor to get a tensor with the desired values. . #collapse-show for i, size in enumerate(SIZES_): _np = np.zeros(size) _np[-1] = 27 _to_search = tf.convert_to_tensor(tf.Variable(_np)) start = time.time() for j, element in enumerate(_to_search): if element == 27: end = time.time() tf_search[i] = end - start . . #collapse for i, size in enumerate(SIZES_): _to_search = np.zeros(size) _to_search[-1] = 27 start = time.time() for j, element in enumerate(_to_search): if element == 27: end = time.time() np_search[i] = end - start . . #collapse for i, size in enumerate(SIZES_): _to_search = torch.zeros(size) _to_search[-1] = 27 start = time.time() for j, element in enumerate(_to_search): if element == 27: end = time.time() torch_search[i] = end - start . . Results . #collapse fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(17,12)) ax[0][0].plot(SIZES, np_creation, label=&quot;Numpy&quot;) ax[0][0].plot(SIZES, torch_creation, label=&quot;PyTorch&quot;) ax[0][0].plot(SIZES, tf_creation, label=&quot;TensorFlow&quot;) ax[0][0].set_title(&quot;Creation&quot;) ax[0][1].plot(SIZES, np_sort, label=&quot;Numpy&quot;) ax[0][1].plot(SIZES, torch_sort, label=&quot;PyTorch&quot;) ax[0][1].plot(SIZES, tf_sort, label=&quot;TensorFlow&quot;) ax[0][1].set_title(&quot;Sorting&quot;) ax[1][0].plot(SIZES_, np_search, label=&quot;Numpy&quot;) ax[1][0].plot(SIZES_, torch_search, label=&quot;PyTorch&quot;) ax[1][0].plot(SIZES_, tf_search, label=&quot;TensorFlow&quot;) ax[1][0].set_title(&quot;Searching&quot;) ax[1][1].plot(SIZES, np_matmul, label=&quot;Numpy&quot;) ax[1][1].plot(SIZES, torch_matmul, label=&quot;PyTorch&quot;) ax[1][1].plot(SIZES, tf_matmul, label=&quot;TensorFlow&quot;) ax[1][1].set_title(&quot;Matmul&quot;) lines, labels = fig.axes[-1].get_legend_handles_labels() fig.legend(lines, labels, loc = &#39;center&#39;, ncol=3, bbox_to_anchor=[0.4, 0.04], fontsize=23) . . &lt;matplotlib.legend.Legend at 0x7fd569d9dac8&gt; . WARNING: Logging before flag parsing goes to stderr. W1207 17:37:31.955271 4738452992 font_manager.py:1282] findfont: Font family [&#39;Franklin Gothic Book&#39;] not found. Falling back to DejaVu Sans. W1207 17:37:31.970885 4738452992 font_manager.py:1282] findfont: Font family [&#39;Franklin Gothic Book&#39;] not found. Falling back to DejaVu Sans. W1207 17:37:32.127390 4738452992 font_manager.py:1282] findfont: Font family [&#39;Franklin Gothic Book&#39;] not found. Falling back to DejaVu Sans. . I am surprised by how well NumPy performs in general. It is faster in everything except in the creation test. One possible explanation is that NumPy is more optimized for the CPU. Unfortunatelly, I do not have a GPU to test this and, even if I had one, NumPy does not support GPU execution natively. . Regarding PyTorch and Tensorflow, I do not see a clear winner. Searching and sorting takes an insane amount of time for Tensorflow compared with the others, but I am running it in eager mode to make the code human-readible. . Which one should I use? . There is no clear answer to that question because it depends on what you want to achieve. If you work in a Data Science field with Python, you would probably need to use NumPy even if you don&#39;t want to (though I can&#39;t image why you wouldn&#39;t want to). . In terms of Deep Learning research, I think PyTorch is more well-suited than TensorFlow because it is easy to learn and to iterate over the models. . Regarding Production-level code, I would consider TensorFlow (with eager mode deactivated) the best one. It is one of the oldest and a lot of services support TensorFlow integration. .",
            "url": "https://xylambda.github.io/blog/python/numpy/pytorch/tensorflow/2020/12/07/torch-vs-numpy-vs-tf.html",
            "relUrl": "/python/numpy/pytorch/tensorflow/2020/12/07/torch-vs-numpy-vs-tf.html",
            "date": " • Dec 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "5 tips for working with time series in Python",
            "content": "This is a repost from my medium article 5 tips for working with time series in Python . . Required knowledge . This post is pretty easy to follow if you already have some basic knowledge of Pandas, NumPy and Python. I will not go into much details with the theoretical stuff but use the resources at the end or ask a question if you need clarification about some particular concept. . Removing noise with the Fourier Transform . It is often the case that we need to study the underlying process that drives a particular time series. To do that, we may want to remove the noise of the time series and analyze the signal. . The Fourier Transform can help us achieve this objective. By moving our time series from the time domain to the frequency domain, we can filter out the frequencies that pollute the data. Then, we just have to apply the inverse Fourier transform to get a filtered version of our time series. . Note: the code presented in this section is a slightly modified version of Steven L. Brunton code. See References section to find the original code and explanation. . &#160;2.1. The code . The following gist contains the necessary code to remove the noise using the Fourier Transform: . def fft_denoiser(x, n_components, to_real=True): &quot;&quot;&quot;Fast fourier transform denoiser. Denoises data using the fast fourier transform. Parameters - x : numpy.array The data to denoise. n_components : int The value above which the coefficients will be kept. to_real : bool, optional, default: True Whether to remove the complex part (True) or not (False) Returns - clean_data : numpy.array The denoised data. References - .. [1] Steve Brunton - Denoising Data with FFT[Python] https://www.youtube.com/watch?v=s2K1JfNR7Sc&amp;ab_channel=SteveBrunton &quot;&quot;&quot; n = len(x) # compute the fft fft = np.fft.fft(x, n) # compute power spectrum density # squared magnitud of each fft coefficient PSD = fft * np.conj(fft) / n # keep high frequencies _mask = PSD &gt; n_components fft = _mask * fft # inverse fourier transform clean_data = np.fft.ifft(fft) if to_real: clean_data = clean_data.real return clean_data . Usage example . Removing noise with the Kalman Filter . With the Fourier Transform we obtain the frequencies that exist in a given time series, but we do not have any information of when these frequencies occur in time. This means that, in its basic form, the Fourier Transform is not the best choice for non-stationary time series. . For example, financial time series are considered non-stationary (although any attempt to prove it statistically is doomed), thus making Fourier a bad choice. . At this point, we can choose to apply the Fourier Transform in a rolling-basis or to go with a Wavelet Transform. But there is a much more interesting algorithm called Kalman Filter. . The Kalman Filter is essentially a Bayesian Linear Regression that can optimally estimate the hidden state of a process using its observable variables. . By carefully selecting the right parameters, one can tweak the algorithm to extract the underlying signal. . 3.1. The code . I created a small library that contains a univariate Kalman Filter that can be used to extract the signal. In the README you will find the particular set of parameters I used. You can also use PyKalman. . 3.2. Example . Dealing with Outliers . Outliers are usually undesirable because they deeply affect our conclusions if we are not careful when dealing with them. For example, the Pearson correlation formula can have a very different result if there are large enough outliers in our data. . Outlier analysis and filtering in time series requires a more sophisticated approach than in normal data, since you cannot use future information to filter past outliers. . One quick way to remove outliers is doing it in a rolling/expanding basis. A common algorithm to find outliers is computing the mean and standard deviation of our data and check which values are n standard deviations above or below the mean (typically, n is set to 3). Those values are then marked as outliers. . The code . The following code allows you to filter outliers using the aforementioned algorithm but in rolling or expanding mode to avoid look-ahead bias. . def basic_filter(data, mode=&#39;rolling&#39;, window=262, threshold=3): &quot;&quot;&quot;Basic Filter. Mark as outliers the points that are out of the interval: (mean - threshold * std, mean + threshold * std ). Parameters - data : pandas.Series The time series to filter. mode : str, optional, default: &#39;rolling&#39; Whether to filter in rolling or expanding basis. window : int, optional, default: 262 The number of periods to compute the mean and standard deviation. threshold : int, optional, default: 3 The number of standard deviations above the mean. Returns - series : pandas.DataFrame Original series and marked outliers. &quot;&quot;&quot; msg = f&quot;Type must be of pandas.Series but {type(data)} was passed.&quot; assert isinstance(data, pd.Series), msg series = data.copy() # rolling/expanding objects pd_object = getattr(series, mode)(window=window) mean = pd_object.mean() std = pd_object.std() upper_bound = mean + threshold * std lower_bound = mean - threshold * std outliers = ~series.between(lower_bound, upper_bound) # fill false positives with 0 outliers.iloc[:window] = np.zeros(shape=window) series = series.to_frame() series[&#39;outliers&#39;] = np.array(outliers.astype(&#39;int&#39;).values) series.columns = [&#39;Close&#39;, &#39;Outliers&#39;] return series . Example . Playing with the parameters you can fine-tune your analysis. Here is an example using the default values of the function. . . The right way to normalize time series data. . Many posts use the classical fit-transform approach with time series as if they could be treated as normal data. As with outliers, you cannot use future information to normalize data from the past unless you are 100% sure the values you are using to normalize are constant over time. . The right way to normalize time series is in a rolling/expanding basis. . The code . I used Sklearn API to create a class that allows you to normalize data avoiding look-ahead bias. Because it inherits BaseEstimator and TransformerMixin it is possible to embed this class in a Sklearn pipeline. . from sklearn.base import BaseEstimator, TransformerMixin class RollingStandardScaler(BaseEstimator, TransformerMixin): &quot;&quot;&quot;Rolling standard Scaler Standardized the given data series using the mean and std commputed in rolling or expanding mode. Parameters - window : int Number of periods to compute the mean and std. mode : str, optional, default: &#39;rolling&#39; Mode Attributes - pd_object : pandas.Rolling Pandas window object. w_mean : pandas.Series Series of mean values. w_std : pandas.Series Series of std. values. &quot;&quot;&quot; def __init__(self, window, mode=&#39;rolling&#39;): self.window = window self.mode = mode # to fill in code self.pd_object = None self.w_mean = None self.w_std = None self.__fitted__ = False def __repr__(self): return f&quot;RollingStandardScaler(window={self.window}, mode={self.mode})&quot; def fit(self, X, y=None): &quot;&quot;&quot;Fits. Computes the mean and std to be used for later scaling. Parameters - X : array-like of shape (n_shape, n_features) The data used to compute the per-feature mean and std. Used for later scaling along the feature axis. y Ignored. &quot;&quot;&quot; self.pd_object = getattr(X, self.mode)(self.window) self.w_mean = self.pd_object.mean() self.w_std = self.pd_object.std() self.__fitted__ = True return self def transform(self, X): &quot;&quot;&quot;Transforms. Scale features of X according to the window mean and standard deviation. Paramaters - X : array-like of shape (n_shape, n_features) Input data that will be transformed. Returns - standardized : array-like of shape (n_shape, n_features) Transformed data. &quot;&quot;&quot; self._check_fitted() standardized = X.copy() return (standardized - self.w_mean) / self.w_std def inverse_transform(self, X): &quot;&quot;&quot;Inverse transform Undo the transform operation Paramaters - X : array-like of shape (n_shape, n_features) Input data that will be transformed. Returns - standardized : array-like of shape (n_shape, n_features) Transformed (original) data. &quot;&quot;&quot; self._check_fitted() unstandardized = X.copy() return (unstandardized * self.w_std) + self.w_mean def _check_fitted(self): &quot;&quot;&quot; Checks if the algorithm is fitted. &quot;&quot;&quot; if not self.__fitted__: raise ValueError(&quot;Please, fit the algorithm first.&quot;) . Example . A flexible way to compute returns. . The last tip is focused on quantitative analysis of financial time series. Working with returns is the first thing you learn as a quant researcher. Hence, it is necessary to have a basic framework to quickly compute log and arithmetic returns in different periods of time. . Also, when filtering financial time series, the ideal procedure filters returns first and then goes back to prices. So you are free to add this step to the code from section 4. . The code . The following gist contains a basic framework to compute returns . &quot;&quot;&quot; Utils functions. &quot;&quot;&quot; import numpy as np import pandas as pd def compute_returns(data, periods=1, log=False, relative=True): &quot;&quot;&quot;Computes returns. Calculates the returns of a given dataframe for the given period. The returns can be computed as log returns or as arithmetic returns Parameters - data : pandas.DataFrame or pandas.Series The data to calculate returns of. periods : int The period difference to compute returns. log : bool, optional, default: False Whether to compute log returns (True) or not (False). relative : bool, optional, default: True Whether to compute relative returns (True) or not (False). Returns - ret : pandas.DataFrame or pandas.Series The computed returns. &quot;&quot;&quot; if log: if not relative: raise ValueError(&quot;Log returns are relative by definition.&quot;) else: ret = _log_returns(data, periods) else: ret = _arithmetic_returns(data, periods, relative) return ret def _arithmetic_returns(data, periods, relative): &quot;&quot;&quot;Arithmetic returns.&quot;&quot;&quot; # to avoid computing it twice shifted = data.shift(periods) ret = (data - shifted) if relative: return ret / shifted else: return ret def _log_returns(data, periods): &quot;&quot;&quot;Log returns.&quot;&quot;&quot; return np.log(data / data.shift(periods)) . Example . These are some of the tips I find more useful in my day-to-day basis. I really hope you find something interesting in this post and, if you find any error or would like to discuss any concept, please leave a comment and I will answer as soon as possible. . References . Callum Ballard — Making Matplotlib Beautiful By Default. | Steven L. Brunton — Denoising Data with FFT [Python]. | Greg Welch, Gary Bishop — An introduction to the Kalman Filter. | Simo Särkkä — Bayesian filtering and smoothing. | Yves-Laurent Kom Samo — Stationarity and Memory in Financial Markets. | Robi Polikar — The Wavelet Tutorial. |",
            "url": "https://xylambda.github.io/blog/python/numpy/pandas/2020/12/06/ts_tips.html",
            "relUrl": "/python/numpy/pandas/2020/12/06/ts_tips.html",
            "date": " • Dec 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Who are you? . Hi! I’m Alejandro, a Data Scientist. I am currently working as quantitative researcher, developing mathematical models to understand the FX market. . What did you study? . My academic background includes a Bachelor of Science in Computer Science and a Big Data minor. The latter as a result of my stay abroad in Netherlands. . I want to know you . I like films and music. I mean, I REALLY like films and music. I play piano and guitar in my free time and I recorded some homemade films (that will never see the light) with my friends. . I also like to read fantasy books and travelling. I’m also interested in politics and science in general. . I want to contact you . You can contact me using the email I have on my GitHub web profile. . How did you make this site? . This site is built with fastpages, an easy to use blogging platform with extra features for Jupyter Notebooks. .",
          "url": "https://xylambda.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://xylambda.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}